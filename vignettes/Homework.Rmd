---
title: "Homework to SA22204174"
author: "SA22204174"
date: "2022-12-13"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Homework to SA22204174}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Homework-2023.09.11

## Question

Use knitr to produce at least 3 examples. For each example, texts should mix with figures and/or tables. Better to have mathematical formulas.

## Answer

The three examples are shown as follows.

### 1. Example of Table and Figure Generated from Random Variables.

We generate 100 samples that follow the standard normal distribution $\mathcal{N}(0,1)$ randomly and compare their distribution trends such as their minimum, maximum, quarter, three quarters, median, and mean with the standard normal distribution $\mathcal{N}(0,1)$.

```{r}
set.seed(1)
data1 <- rnorm(100)
samples <- summary(data1)
population <- c(-Inf, qnorm(0.25), qnorm(0.5), 0, qnorm(0.75), +Inf)
table1 <- cbind(samples, population)
knitr::kable(table1)
```

Then we compare the frequency distribution histogram of the samples with the density function of the standard normal distribution,
$$f(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}.$$

```{r}
x <- seq(-3, 3, 0.1)
#plot(x, dexp(x), ty="l", main="指数分布", xlab="x", ylab="f(x)")
hist(data1, freq = FALSE, main = 'Histogram of samples from N(0,1)', xlim = c(-3,3), xlab = '')
lines(x, dnorm(x), col="red")
```

### 2. Example of Table and Figure Generated form Data

This famous (Fisher's or Anderson's) iris data set gives the measurements in centimeters of the variables sepal length and width and petal length and width, respectively, for 50 flowers from each of 3 species of iris. The species are Iris setosa, versicolor, and virginica.

iris is a data frame with 150 cases (rows) and 5 variables (columns) named Sepal.Length, Sepal.Width, Petal.Length, Petal.Width, and Species. We output its first 6 rows in table.

```{r}
d <- iris
knitr::kable(d[1:6,])
```

We made frequency histogram and density curve for the length of sepals of each iris flower species.

```{r}
#par(mfrow=c(2,2))
hist(d[1:50, "Sepal.Length"], freq = FALSE, main = 'Histogram of setosa', xlim = c(4,6), xlab = '')
lines(density(d[1:50, "Sepal.Length"]), col = "red")
hist(d[51:100, "Sepal.Length"], freq = FALSE, main = 'Histogram of versicolor', xlim = c(4,8), xlab = '')
lines(density(d[51:100, "Sepal.Length"]), col = "red")
hist(d[101:150, "Sepal.Length"], freq = FALSE, main = 'Histogram of virginica', xlim = c(4,9), xlab = '')
lines(density(d[101:150, "Sepal.Length"]), col = "red")
```

### 3. Example of Table and Figure Inserted in R Markdown

We typed the table in Example 2 in the document in LaTeX format and markdown format. The code is shown in the inserted image.



\begin{table}[ht]
  \centering
  \begin{tabular}{rrrrrl}
    \hline
    & Sepal.Length & Sepal.Width & Petal.Length & Petal.Width & Species \\
    \hline
    1 & 5.10 & 3.50 & 1.40 & 0.20 & setosa \\
    2 & 4.90 & 3.00 & 1.40 & 0.20 & setosa \\
    3 & 4.70 & 3.20 & 1.30 & 0.20 & setosa \\
    4 & 4.60 & 3.10 & 1.50 & 0.20 & setosa \\
    5 & 5.00 & 3.60 & 1.40 & 0.20 & setosa \\
    6 & 5.40 & 3.90 & 1.70 & 0.40 & setosa \\
    \hline
  \end{tabular}
\end{table}

And we present the table in this HTML with code in markdown form.

| Sepal.Length | Sepal.Width | Petal.Length | Petal.Width | Species |
|-------------:|------------:|-------------:|------------:|:--------|
| 5.10 | 3.50 | 1.40 | 0.20 | setosa |
| 4.90 | 3.00 | 1.40 | 0.20 | setosa |
| 4.70 | 3.20 | 1.30 | 0.20 | setosa |
| 4.60 | 3.10 | 1.50 | 0.20 | setosa |
| 5.00 | 3.60 | 1.40 | 0.20 | setosa |
| 5.40 | 3.90 | 1.70 | 0.40 | setosa |



# Homework-2023.09.18

## Question

### Question 1. 

课后习题: 利用逆变换发复现函数sample的部分功能(replace=TRUE), my.sample <- function().

### Question 2. 

Exercises 3.2, 3.7, 3.9, 3.10 (pages 94-96, Statistical Computing with R).

## Answer

### 1. 课后习题.

**Problem.** 利用逆变换发复现函数sample的部分功能(replace=TRUE), my.sample <- function().

**Solution.** We define a function named "my.sample", which repeats some of the functionality of the function sample, through the inverse transform method. As for the new function "my.sample", we input the range of values of random variables, the probability corresponding to each value and the number of random samples to be generated, and the function outputs the corresponding number of random samples with replacement.

```{r}
my.sample <- function(x, n, p = 0){
#  cat(p,'\n')
  if(length(p) == 1){
    cp <- seq(1/length(x), 1, 1/length(x))
  }else{
    cp <- cumsum(p)
  }
#  cat(cp,'\n')
  U <- runif(n)
  r <- x[findInterval(U,cp)+1]
  return(r)
}
```

We take some examples to show the functionality of function "my.sample". In each example, we compare the frequency of random samples generated by the function with the distribution of the random variable.

Example.1. We generate 2000 random samples which take values in $(1,2,3)$ with averaged probability.
```{r}
set.seed(1)
example1.sample <- my.sample(x = 1:3, n = 2000)
example1.ct <- as.vector(table(example1.sample))
example1.ct/sum(example1.ct)/(1/3)
```

Example.2. We generate 5000 random samples which take values in $(1,2,3,4)$ with probability $(0.1,0.2,0.3,0.4)$.
```{r}
set.seed(1)
example2.sample <- my.sample(x = 1:4, n = 5000, p = seq(0.1,0.4,0.1))
example2.ct <- as.vector(table(example2.sample))
example2.ct/sum(example2.ct)/(seq(0.1,0.4,0.1))
```

Example.1. We generate 1000 random samples which take values in $(1,2,10)$ with probability $(0.1,0.7,0.2)$.
```{r}
set.seed(1)
example3.sample <- my.sample(x = c(1,2,10), n = 1000, p = c(0.1,0.7,0.2))
example3.ct <- as.vector(table(example3.sample))
example3.ct/sum(example3.ct)/(c(0.1,0.7,0.2))
```

It has shown that the distribution of the random sample generated by the function "my.sample" is close to the theoretical distribution.

### 2. Exercise 3.2. (Page 94, Statistical Computing with R). 

**Problem.** The standard Laplace distribution has density $f(x)=\frac{1}{2}e^{-|x|},\ x\in\mathbb{R}$. Use the inverse transform method to generate a random sample of size 1000 from this distribution. Use one of the methods shown in this chapter to compare the generated sample to the target distribution.

**Solution.** Suppose $X$ is a random variable with density
$$f(x)=\frac{1}{2}e^{-|x|}=\frac{1}{2}e^{x}I_{(x\leq0)}+\frac{1}{2}e^{-x}I_{(x>0)}.$$
So the distribution function of $X$ is
$$F(x)=\int_{-\infty}^xf(t)dt=\frac{1}{2}e^xI_{(x\leq0)}+(1-\frac{1}{2}e^{-x})I_{(x>0)}.$$
And the inverse function of $F(x)$ is
$$F^{-1}(y)=\log(2y)I_{(0\leq y\leq\frac{1}{2})}-\log(2(1-y))I_{(\frac{1}{2}<y\leq1)}.$$
We also suppose random variable $U\sim$Uniform(0,1), then we have that $F^{-1}(U)$ has the same distribution as $X$.

Now we use the inverse transform mathod to generate a random sample of size 1000 from the target distribution.
```{r}
# the inverse function of distribution function F
F.inverse <- function(x){
  if(0 <= x && x <= 0.5){
    y <- log(2*x)
  }else{
    y <- -log(2*(1-x))
  }
  return(y)
}
```
```{r}
# inverse transform method
set.seed(1)
n <- 1000
u <- runif(n)
x.sample.1 <- sapply(u, F.inverse)
```

Then we compare the generated sample to the target diatribution.
```{r}
x.1 <- seq(-10,10,0.1)
hist(x.sample.1, prob = TRUE, xlim = c(-10,10), ylim = c(0,0.5), xlab = "x", main =  "Histogram of x")
lines(x.1, exp(-abs(x.1))/2)
```

From the figure we conclude that the distribution of samples roughly fit the target distribution.

### 3. Exercise 3.7. (Page 94, Statistical Computing with R). 

**Problem.** Write a function to generate a random sample of size n from the Beta$(a, b)$ distribution by the acceptance-rejection method. Generate a random sample of size 1000 from the Beta(3,2) distribution. Graph the histogram of the sample with the theoretical Beta(3,2) density superimposed.

**Solution.** We use the acceptance-rejection method to generate the random sample of size 1000. Suppose random variable $X\sim$Beta$(a,b)$, so the density of $X$ is
$$f(x)=\frac{1}{\mathrm{Beta}(a,b)}x^{a-1}(1-x)^{b-1}I_{(0,1)}(x)=\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}x^{a-1}(1-x)^{b-1}I_{(0,1)}(x).$$
We also suppose random variable $Y\sim$Uniform(0,1), whose density is $g(x)=I_{(0,1)}(x)$. Then we have
$$\frac{f(x)}{g(x)}=\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}x^{a-1}(1-x)^{b-1}\leq\frac{\Gamma(a+b)}{\Gamma(a)\Gamma(b)}\triangleq c.$$
So we define
$$\rho(Y)=\frac{f(Y)}{cg(Y)}=x^{a-1}(1-x)^{b-1}I_{(0,1)}(x).$$

Based on the acceptance-rejection algorithm, our algorithm is shown below:

Step 1. Generate random numbers $U\sim$Uniform(0,1) and $Y\sim g(\cdot)$, which means that $Y\sim$Uniform(0,1),

Step 2. If $U\leq\rho(Y)$, then accept $Y$ and stop (return $X=Y$); otherwise reject $Y$ and continue.

```{r}
# preparation
a <- 3
b <- 2
n <- 1000
accept <- 0
x.sample.2 <- numeric(n)

# acceptance-rejection algorithm
set.seed(1)
while(accept < n){
  u <- runif(1)
  x <- runif(1) # random variable from g(.)
  if(x^(a-1)*(1-x)^(b-1) > u){
    # we accept x
    accept <- accept + 1
    x.sample.2[accept] <- x
  }
}
```

Then we graph the histogram of the sample with the theoretical Beta(3,2) density superimposed.
```{r}
x.2 <- seq(0,1,0.02)
hist(x.sample.2, prob = TRUE, xlim = c(0,1), ylim = c(0,2), xlab = "x", main = "Histogram of x")
lines(x.2, x.2^(a-1)*(1-x.2)^(b-1)*gamma(a+b)/(gamma(a)*gamma(b)))
```

From the figure we conclude that the distribution of samples roughly fit the target distribution.

### 4. Exercise 3.9. (Page 95, Statistical Computing with R). 

**Problem.** The rescaled Epanechnikov kernel [85] is a symmetric density function
$$f_e(x)=\frac{3}{4}(1-x^2),\quad|x|\leq1.$$
Devroye and Gy\¨orfi [71, p. 236] give the following algorithm for simulation from this distribution. Generate iid $U_1,U_2,U_3\ \sim$Uniform(−1, 1). If $|U_3|\geq|U_2|$ and $|U_3|\geq|U_1|$, deliver $U_2$; otherwise deliver $U_3$. Write a function to generate random variates from $f_e$, and construct the histogram density estimate of a large simulated random sample.

**Solution.** Note that
$$g(u_1,u_2,u_3)=u_2I_{(|u_3|\geq|u_1|\ \mathrm{and}\ |u_3|\geq|u_2|)}+u_3(1-I_{(|u_3|\geq|u_1|\ \mathrm{and}\ |u_3|\geq|u_2|)}).$$
And we generate function $fe$ according to the description of the question.
```{r}
fe <- function(n){
  x <- numeric(n)
  u1 <- runif(n, -1, 1)
  u2 <- runif(n, -1, 1)
  u3 <- runif(n, -1, 1)
  for(i in 1:n){
    if((abs(u3[i]) >= abs(u2[i])) && (abs(u3[i]) >= abs(u1[i]))){
      x[i] <- u2[i]
    }else x[i] <- u3[i]
  }
  return(x)
}
```

Then we construct the histogram density estimate of a simulated random sample of size 1000.
```{r}
set.seed(1)
n <- 1000
x.sample.3 <- fe(n)

x.3 <- seq(-1,1,0.05)
hist(x.sample.3, prob = TRUE, xlim = c(-1,1), ylim = c(0, 0.8), xlab = "x", main = "Histogram of x")
lines(x.3, (1-x.3^2)^2*3/4)
```

From the figure we conclude that the distribution of samples roughly fit the target distribution.

### 5. Exercise 3.10. (Page 95, Statistical Computing with R). 

**Problem.** Prove that the algorithm given in Exercise 3.9 generates variates from the density $f_e$ (3.10).

**Proof.** The density of $U_1,U_2,U_3$ is $f_U(u)=\frac{1}{2}I_{(-1,1)}(u)$, and the distribution function of them is $F_U(u)=\frac{1}{2}(x+1)I_{(-1,1)}(u)$. Because of the independence of $U_1,U_2,U_3$, the joint density of $(U_1,U_2,U_3)$ is
$$f_{(U_1,U_2,U_3)}(u_1,u_2,u_3)=f_U(u_1)f_U(u_2)f_U(u_3)=\frac{1}{8}\prod_{i=1}^3I_{(-1,1)}(u_i).$$

Note that $f(x)\triangleq f_{g(U_1,U_2,U_3)}(x)$. For $x\in(-\infty,-1]\cup[1,+\infty)$, $f(x)=0$. And for $x\in(-1,1)$,
$$P(g(U_1,U_2,U_3)\leq x)=P(U_2\leq x, g(U_1,U_2,U_3)=U_2)+P(U_3\leq x, g(U_1,U_2,U_3)=U_3).$$

For the first term, we have

\begin{equation*}
  \begin{aligned}
    &P(U_2\leq x, g(U_1,U_2,U_3)=U_2)=P(U_2\leq x,|U_3|\geq|U_2|,|U_3|\geq|U_1|)\\
    =&\frac{1}{8}\int_{\mathbb{R}}I_{(-1,1)}(u_3)\left(\int_{-|u_3|}^{\min\{|u_3|,x\}}I_{(-1,1)}(u_2)du_2\right)\left(\int_{-|u_3|}^{|u_3|}I_{(-1,1)}(u_1)du_1\right)du_3\\
    =&\frac{1}{4}\int_{-1}^1(\min\{|u_3|,x\}+|u_3|)|u_3|du_3=\frac{1}{2}\int_{0}^1(\min\{u_3,x\}+u_3)u_3du_3\\
    =&\frac{1}{2}\int_{0}^x2u_3^2du_3+\frac{1}{2}\int_x^1(u_3+x)u_3du_3=\frac{1}{2}\int_0^xu_3^2du_3+\frac{1}{2}\int_0^1u_3^2du_3+\frac{x}{2}\int_x^1u_3du_3\\
    =&\frac{1}{6}x^3+\frac{1}{6}+\frac{x}{4}(1-x^2)=-\frac{x^3}{12}+\frac{x}{4}+\frac{1}{6}.
  \end{aligned}
\end{equation*}

And for the second term, we have
\begin{equation*}
  \begin{aligned}
    &P(U_3\leq x, g(U_1,U_2,U_3)=U_3)=P(U_3\leq x)-P(U_3\leq x, g(U_1,U_2,U_3)=U_2)\\
    =&P(U_3\leq x)-P(U_3\leq x,|U_3|\geq|U_2|,|U_3|\geq|U_1|)\\
    =&\frac{x+1}{2}-\frac{1}{8}\int_{-\infty}^xI_{(-1,1)}(u_3)\left(\int_{-|u_3|}^{|u_3|}I_{(-1,1)}(u_2)du_2\right)\left(\int_{-|u_3|}^{|u_3|}I_{(-1,1)}(u_1)du_1\right)du_3\\
    =&\frac{x+1}{2}-\frac{1}{2}\int_{-1}^x|u_3|^2du_3=\frac{x+1}{2}-\frac{1}{2}\int_0^1u_3^2du_3-\frac{1}{2}\int_0^xu_3^2du_3\\
    =&\frac{x+1}{2}-\frac{1}{6}-\frac{x^3}{6}=-\frac{x^3}{6}+\frac{x}{2}+\frac{1}{3}.
  \end{aligned}
\end{equation*}

So we finally obtain that
$$P(g(U_1,U_2,U_3)\leq x)=-\frac{x^3}{4}+\frac{3x}{4}+\frac{1}{2}.$$
And the density of $g(U_1,U_2,U_3)$ is
$$f(x)=\frac{dP(g(U_1,U_2,U_3)\leq x)}{dx}=-\frac{3x^2}{4}+\frac{3}{4}=\frac{3}{4}(1-x^2),$$
which equals the target density $f_e$.



# Homework-2023.09.25

## Question

### Question 1.

- Proof that what value $\rho=\frac{l}{d}$ should take to minimize the asymptotic variance of $\hat{\pi}$? ($m\sim B(n,p)$, using $\delta$ method).

- Take three different values of $\rho$ ($0\leq\rho\leq1$, including $\rho_{\min}$) and use Monte Carlo simulation to verify your answer. ($n=10^6$, Number of repeated simulations $K=100$)

### Question 2.

Exercises 5.6, 5.7 (pages 149-151, Statistical Computing with R)

## Answer

### Question 1.1.

**Problem.**Proof that what value $\rho=\frac{l}{d}$ should take to minimize the asymptotic variance of $\hat{\pi}$? ($m\sim B(n,p)$, using $\delta$ method).

**Proof.** According to the definition of random variables $X$ and $Y$ we have known that $X\sim U(0,\frac{d}{2})$ and $Y\sim U(0,\frac{\pi}{2})$. Then we obtain the expectation and variance of the random variable $I_{\left\{\frac{l}{2}\sin Y\geq X\right\}}$ that
$$p\triangleq\mathbb{E}\left[I_{\left\{\frac{l}{2}\sin Y\geq X\right\}}\right]=\mathrm{P}\left(\frac{l}{2}\sin Y\geq X\right)=\frac{2l}{d\pi}=\frac{2\rho}{\pi},$$
and
\begin{align*}
  \sigma^2\triangleq&\mathrm{Var}\left(I_{\left\{\frac{l}{2}\sin Y\geq X\right\}}\right)=\mathbb{E}\left[\mathbb{E}\left[\left.(I_{\left\{\frac{l}{2}\sin Y\geq X\right\}}-p)^2\right|Y\right]\right]\\
=&\mathbb{E}\left[p^2\frac{2}{d}\left(\frac{d}{2}-\frac{l}{2}\sin Y\right)+(1-p)^2\frac{2}{d}\cdot\frac{l}{2}\sin Y\right]\\
=&p^2-p^2\frac{l}{d}\mathbb{E}[\sin Y]+(1-p)^2\frac{l}{d}\mathbb{E}[\sin Y]\\
=&p^2+(1-2p)\rho\mathbb{E}[\sin Y]=p^2+\frac{2}{\pi}(1-2p)\rho\\
=&\frac{4\rho^2}{\pi^2}+\frac{2\rho}{\pi}\cdot\left(1-\frac{4\rho}{\pi}\right)=\frac{2\rho}{\pi}-\frac{4\rho^2}{\pi^2}.
\end{align*}
Combining the CLT with the definition of random variable $m\triangleq\sum_{i=1}^nI_{\left\{\frac{l}{2}\sin Y_i\geq X_i\right\}}\sim B(n,p)$, we obtain that
$$\sqrt{n}\left(\frac{m}{n}-p\right)\xrightarrow[n\to\infty]{d}N(0,\sigma^2).$$

Because $\frac{m}{n}=\frac{2l}{d\hat{\pi}}=\frac{2\rho}{\hat{\pi}}$, we have $\hat{\pi}=\frac{2n\rho}{m}$. Define $g(t)=\frac{2\rho}{t}$, then $g'(t)=-\frac{2\rho}{t^2}$. And from the $\delta$-method we have
$$\sqrt{n}\left(g\left(\frac{m}{n}\right)-g(p)\right)\xrightarrow[n\to\infty]{d}N(0,\left(g'(p)\right)^2\sigma^2),$$
which means that
$$\sqrt{n}(\hat{\pi}-\pi)\xrightarrow[n\to\infty]{d} N(0,\tau^2),$$
where
$$\tau^2=\left(\frac{2\rho}{p^2}\right)^2\left(\frac{2\rho}{\pi}-\frac{4\rho^2}{\pi^2}\right)=\left(\frac{\pi^2}{2\rho}\right)^2\frac{2\rho}{\pi}\left(1-\frac{2\rho}{\pi}\right)=\frac{\pi^3}{2}\left(\frac{1}{\rho}-\frac{2}{\pi}\right).$$
Because $\tau^2$ decreases with the increase of $\rho\in(0,1]$, we fine that when $\rho=\rho_{\min}=1$ the variance of $\hat{\pi}$ reaches its minimum value $\tau^2_{\min}=\frac{\pi^2(\pi-2)}{2\pi}$.

### Question 1.2.

**Problem.** Take three different values of $\rho$ ($0\leq\rho\leq1$, including $\rho_{\min}$) and use Monte Carlo simulation to verify your answer. ($n=10^6$, Number of repeated simulations $K=100$)

**Solution.** We firstly define the function to generate $\hat{\pi}$.

```{r}
pihat <- function(rho){
  d <- 1
  l <- rho
  n <- 1e6
  X <- runif(n,0,d/2)
  Y <- runif(n,0,pi/2)
  pi.hat <- 2*l/d/mean(l/2*sin(Y)>X)
  return(pi.hat)
}
```

Then we take $\rho=0.1,0.5,1$ to verify the answer in the previous question.
```{r}
K <- 100
rho <- c(0.1, 0.5, 1)
pihat.sample <- matrix(0, nrow = length(rho), ncol = K)

# obtain pihat
set.seed(1)
pihat.sample[1,] <- sapply(rep(rho[1], times = K), pihat)
pihat.sample[2,] <- sapply(rep(rho[2], times = K), pihat)
pihat.sample[3,] <- sapply(rep(rho[3], times = K), pihat)
```
```{r}
# calculate the standard deviation
pihat.var <- apply(pihat.sample, 1, var)
```
```{r}
names(pihat.var) <- c("rho=0.1", "rho=0.5", "rho=1")
#knitr::kable(variance)
pander::pander(pihat.var)
```

Comparing the variances of the three cases, we obtain that when $\rho=1$ the variance of $\rho$ reaches its minimum, which is consistent with our previous conclusion.

### Exercises 5.6. (pages 150, Statistical Computing with R).

**Problem.** In Example 5.7 the control variate approach was illustrated for Monte Carlo integration of
$$\theta=\int_0^1e^xdx.$$
Now consider the antithetic variate approach. Compute $Cov(e^U,e^{1-U})$ and $Var(e^U,e^{1-U})$, where $U\sim$Uniform(0,1). What is the percent reduction in variance of $\hat{\theta}$ that can be achieved using antithetic variates (compared with simple MC)?

**Solution.** Because random variable $e^U$ and $e^{1-U}$ have the same distribution, we have that
$$\mathbb{E}\left[e^{1-U}\right]=\mathbb{E}\left[e^U\right]=\int_0^1e^udu=e-1,$$
and
$$\mathbb{E}\left[e^{2-2U}\right]=\mathbb{E}\left[e^{2U}\right]=\int_0^1e^{2u}du=\frac{1}{2}(e^2-1).$$
So we obtain the covariance of $e^U$ and $e^{1-U}$ that
$$Cov\left(e^U,e^{1-U}\right)=\mathbb{E}\left[e^U\cdot e^{1-U}\right]-\mathbb{E}\left[e^U\right]\mathbb{E}\left[e^{1-U}\right]=e-(e-1)^2=-e^2+3e-1,$$
as well as the variance of both of them that
$$Var\left(e^{1-U}\right)=Var\left(e^U\right)=\mathbb{E}\left[e^{2U}\right]-\mathbb{E}^2\left[e^U\right]=\frac{1}{2}(e^2-1)-(e-1)^2\\=\frac{1}{2}(e-1)(e+1-2e+2)=\frac{1}{2}(e-1)(3-e).$$
We can also calculate that
$$=Var\left(e^U+e^{1-U}\right)=Var(e^U)+Var(e^{1-U})+2Cov(e^U,e^{1-U})\\
=(e-1)(3-e)+2(-e^2+3e-1)=-3e^2+10e-5.$$
From the computation
```{r}
-exp(2)+3*exp(1)-1
(-3*exp(2)+10*exp(1)-5)/4
```

We have that $Cov(e^U,e^{1-U})=-0.2342106$ and $Var(e^U+e^{1-U})=0.003912497$.

We suppose random variable $U,V\sim$Uniform(0,1) are identically independent distributed and define $\hat{\theta}_s=\frac{1}{2}(e^U+e^V)$ is the simple MC estimator and $\hat{\theta}_a=\frac{1}{2}(e^U+e^{1-U})$ is the antithetic estimator. Then we have
$$Var(\hat{\theta}_s)=\frac{1}{4}Var\left(e^U+e^V\right)=\frac{1}{2}Var\left(e^U\right)=\frac{1}{4}(e-1)(3-e),$$
and
$$Var(\hat{\theta}_a)=\frac{1}{4}Var\left(e^U+e^{1-U}\right)
=\frac{1}{4}\left((e-1)(3-e)+2(-e^2+3e-1)\right).$$
The percent reduction in variance of $\hat{\theta}$ that can be achieved using antithetic variates is
$$\frac{Var(\hat{\theta}_s)-Var(\hat{\theta}_a)}{Var(\hat{\theta}_s)}=\frac{2(e^2-3e+1)}{(e-1)(3-e)}.$$
Now we calculate the number.
```{r}
reduction.theoretical <- 2*(exp(2)-3*exp(1)+1)/((exp(1)-1)*(3-exp(1)))
cat("The theoretical percent reduction is", reduction.theoretical, "." ,'\n')
```

### Exercise 5.7. (pages 150, Statistical Computing with R).

**Problem.** Refer to Exercise 5.6. Use a Monte Carlo simulation to estimate $\theta$ by the antithetic variate approach and by the simple Monte Carlo method. Compute an empirical estimate of the percent reduction in variance using the antithetic variate. Compare the result with the theoretical value from Exercise 5.6.

**Solution. ** We firstly use the MC method with samples $n=10000$ to generate $\hat{\theta}_s$ and $\hat{\theta}_a$ and the number of repeated simulations $K=1000$).
```{r}
MC_s <- function(n){
  return(mean(exp(runif(n))))
}
MC_a <- function(n){
  u <- runif(n/2)
  v <- 1-u
  return(mean(exp(u)+exp(v))/2)
}
```
```{r}
n <- 10000
K <- 1000

set.seed(1)
theta_s <- sapply(rep(n,times = K), MC_s)
theta_a <- sapply(rep(n,times = K), MC_a)
```

Then we compute the empirical estimate of the percent reduction in variance using the antithetic variate.
```{r}
var_s <- var(theta_s)
var_a <- var(theta_a)
reduction.empirical <- (var_s-var_a)/var_s
cat("The empirical percent reduction is",reduction.empirical, "." ,'\n')
```
```{r}
reduction <- c(reduction.theoretical, reduction.empirical)
names(reduction) <- c("theoretical", "empirical")
pander::pander(reduction)
```

In conclusion, the empirical result is close to the theoretical value.



# Homework-2023.10.09

## Question

### Question 1.



$Var(\hat{\theta}^M)=\frac{1}{Mk}\sum_{i=1}^k\sigma_i^2+Var(\theta_I)=Var(\hat{\theta}^S)+Var(\theta_I)$, where $\theta_i=E[g(U)|I=i]$, $\sigma_i^2=Var(g(U)|I=i)$ and $I$ takes uniform distribution over $\{1,\dots,k\}$.

Proof that if $g$ is a continuous function over $(a, b)$, then $Var(\hat{\theta}^S)/Var(\hat{\theta}^M)\to0$ as $b_i-a_i\to0$ for all $i=1,\dots,k$.

### Question 2.

- Exercises 5.13, 5.14, 5.15 (pages 149-151, Statistical Computing with R).

- Exercises 6.5, 6.A (pages 180-181, Statistical Computing with R).

## Answer

### Question 1.

**Proof.** For $i=1,\dots,k$, when $I=i$, $U\sim$Uniform$(a_i,b_i)$. So we have that
$$\theta_i=\frac{1}{b_i-a_i}\int_{a_i}^{b_i}g(u)du,\quad \sigma_i^2=\frac{1}{b_i-a_i}\int_{a_i}^{b_i}g^2(u)du-\theta_i^2.$$
Then we obtain that
$$Var(\hat{\theta}^S)=\frac{1}{Mk}\sum_{i=1}^k\frac{1}{b_i-a_i}\int_{a_i}^{b_i}g^2(u)du-\frac{1}{Mk}\sum_{i=1}^k\theta_i^2,$$
and
$$Var(\theta_I)=\frac{1}{k}\sum_{i=1}^k\theta_i^2-\left(\frac{1}{k}\sum_{i=1}^k\theta_i\right)^2.$$

Because $g$ is a continuous function over $(a, b)$, from the first mean value theorem for integration, there exists $x_i,y_i\in[a_i,b_i]$ which satisfies that
$$\theta_i=\frac{1}{b_i-a_i}\int_{a_i}^{b_i}g(u)du=g(x_i),\quad \frac{1}{b_i-a_i}\int_{a_i}^{b_i}g^2(u)du=g^2(y_i).$$
Because $(b_i-a_i)=0,\forall i=1,\dots,k$, from the definition of Riemann Integration, we obtain that as $(b_i-a_i)=0,\forall i=1,\dots,k$, 
$$\frac{1}{k}\sum_{i=1}^k\theta_i\xrightarrow{}\int_{a}^bg(u)du,\ \frac{1}{k}\sum_{i=1}^k\theta_i^2\xrightarrow{}\int_{a}^bg^2(u)du,$$
and
$$\frac{1}{k}\sum_{i=1}^k\frac{1}{b_i-a_i}\int_{a_i}^{b_i}g^2(u)du\xrightarrow{}\int_{a}^bg^2(u)du.$$

So we have
$$Var(\hat{\theta}^S)=\frac{1}{Mk}\sum_{i=1}^k\frac{1}{b_i-a_i}\int_{a_i}^{b_i}g^2(u)du-\frac{1}{Mk}\sum_{i=1}^k\theta_i^2\xrightarrow{}\frac{1}{M}\left(\int_{a}^bg^2(u)du-\int_{a}^bg^2(u)du\right)=0,$$
and
$$Var(\theta_I)=\frac{1}{k}\sum_{i=1}^k\theta_i^2-\left(\frac{1}{k}\sum_{i=1}^k\theta_i\right)^2\xrightarrow{}\int_{a}^bg^2(u)du-\left(\int_{a}^bg(u)du\right)^2\geq0,$$
as $(b_i-a_i)=0,\forall i=1,\dots,k$.

If $g(u)\equiv C$, where $C$ is any constant on $\mathbb{R}$, we have that
$$Var(\hat{\theta}^S)=\frac{C^2}{M}-\frac{C^2}{M}=0,\ Var(\theta_I)=C^2-C^2=0,$$
So
$$Var(\hat{\theta}^M)=Var(\hat{\theta}^S)+Var(\theta_I)=0.$$
Then $Var(\hat{\theta}^S)/Var(\hat{\theta}^M)$ is not exist.

If $g(u)$ is not identical to a constant, $\int_{a}^bg^2(u)du-\left(\int_{a}^bg(u)du\right)^2>0$. Then we have that as $(b_i-a_i)=0,\forall i=1,\dots,k$,
$$\frac{Var(\hat{\theta}^S)}{Var(\hat{\theta}^M)}=\frac{Var(\hat{\theta}^S)}{Var(\hat{\theta}^S)+Var(\theta_I)}\xrightarrow{}\frac{0}{\int_{a}^bg^2(u)du-\left(\int_{a}^bg(u)du\right)^2}=0.$$

### Exercises 5.13. (pages 151, Statistical Computing with R).

**Problem.** Find two importance functions $f_1$ and $f_2$ that are supported on $(1,\infty)$ and are 'close' to
$$g(x)=\frac{x^2}{\sqrt{2\pi}}e^{-\frac{x^2}{2}},\ x>1.$$
Which of your two importance functions should produce the smaller variance in estimating
$$\int_1^{\infty}\frac{x^2}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx$$
by importance sampling? Explain.

**Solution.** We suppose the first importance function is the density of standard normal distribution $N(0,1)$ that
$$f_1(x)=\frac{1}{\sqrt{2\pi}}e^{-\frac{x^2}{2}},$$
and the second important function is the density of Gamma distribution $\Gamma(\lambda,a)$ with $\lambda=1,a=3$ that
$$f_2(x)=\frac{\lambda^a}{\Gamma(a)}x^{a-1}e^{-\lambda x}I_{(x>0)}=\frac{1}{\Gamma(3)}x^2e^{-x}I_{(x>0)}=\frac{1}{2}x^2e^{-x}I_{(x>0)}.$$

Firstly we display the figure of $g(x), f_1(x), f_2(x)$.
```{r}
g <- function(x) x^2 * exp(-x^2/2)/sqrt(2 * pi)*(x>1)
f1 <- function(x) dnorm(x, mean = 0, sd = 1)
f2 <- function(x) dgamma(x, shape = 3, rate = 1)
```

```{r}
x <- seq(1, 8, 0.01)
plot(x, g(x), type = "l", ylim = c(0, 0.4), ylab = "", main = "Figure of g(x), f1(x), f2(x)")
lines(x, f1(x), lty = 2)
lines(x, f2(x), lty = 3)
legend("topright", inset = 0.02, legend = c(expression(g(x)==x^2*e^{-x^2/2}/sqrt(2*pi)), expression(f[1](x)==e^{-x^2/2}/sqrt(2*pi)), expression(f[2](x)==x^2*e^{-x}/2)), lty = 1:3)
```

From the figure we find that the two importance functions $f_1$ and $f_2$ are 'close' to $g(x)$ on $(1,\infty)$.

In order to discuss which of the two importance functions should produce the smaller variance, we display the figure of $g(x)/f_1(x)$ and $g(x)/f_2(x)$ to compare the rates of $g(x)/f(x)$.

```{r}
x <- seq(1,5,0.01)
plot(x, g(x)/f1(x), type = "l", lty = 2, ylim = c(0,3), ylab = "", main = "Figure of g(x)/f1(x) and g(x)/f2(x)")
lines(x, g(x)/f2(x), lty = 3)
legend("topright", inset = 0.02, legend = c(expression(f[1](x)==e^{-x^2/2}/sqrt(2*pi)), expression(f[2](x)==x^2*e^{-x}/2)), lty = 2:3)
```

From the figure we conclude that $g(x)/f_1(x)$ tends to infinity fast, and $g(x)/f_2(x)$ tends to 0 slowly. So $g(x)/f_2(x)$ is more close to a constant and $f_2$ performs better than $f_1$ in the importance sampling procedure.

### Exercises 5.14. (pages 151, Statistical Computing with R).

**Problem.** Obtain a Monte Carlo estimate of
$$\int_1^{\infty}\frac{x^2}{\sqrt{2\pi}}e^{-\frac{x^2}{2}}dx$$
by importance sampling.

**Solution.** We use importance sampling to calculate the integration in the case that the importance functions are $f_1(x)$ and $f_2(x)$ respectively.
```{r}
m <- 10000
theta.hat <- sd.theta.hat <- numeric(2)

# the importance function is f1
set.seed(1)
x <- rnorm(m)
theta.hat[1] <- mean(g(x)/f1(x))
sd.theta.hat[1] <- sd(g(x)/f1(x))

# the importance function is f2
set.seed(1)
x <- rgamma(m,shape = 3,rate = 1)
theta.hat[2] <- mean(g(x)/f2(x))
sd.theta.hat[2] <- sd(g(x)/f2(x))

# result reporting
rbind(theta.hat, sd.theta.hat)
```

The simulation indicates that $f_2$ produce smaller variance than $f_1$, which is consistent with our conclusion of the last question. So we take the importance function $f=f_2$ and get an estimate of $\hat{\theta}=0.401$.

### Exercises 5.15. (pages 151, Statistical Computing with R).

**Problem.** Obtain the stratified importance sampling estimate in Example 5.13 and compare it with the result of Example 5.10.

**Solution.**  In Example 5.10 our best result was obtained with importance function $f(x)=e^{-x}/(1-e^{-1}),0<x<1$.From 10000 replicates we obtained the estimate $\hat{\theta}=0.5257801$ and an estimated standard error $0.0970314$. Now divide the interval to five subintervals, $(j/5,(j+1)/5),j=0,1,\dots,4$. Then on the $j^{th}$ subinterval variables are generated from the density
$$f_j(x)=\frac{5e^{-x}}{1-e^{-1}}\cdot I_{\left(F^{-1}(\frac{j-1}{5})<x<F^{-1}(\frac{j}{5})\right)},$$
where $F^{-1}(x)=-\log(1-(1-e^{-1})t)$, for $j=1,\dots,5$.

We suppose $U_j\sim$Uniform$\left(\frac{j-1}{5},\frac{j}{5}\right),j=1,\dots,5$ and $\int_{0}^{X_j}\frac{e^{-t}}{1-e^{-1}}dt=\frac{1-e^{-X}}{1-e^{-1}}=U_j\Leftrightarrow X_j=-\log(1-(1-e^{-1})U_j)$. Then $X_j$ have the density $f_j(x)$.

First we get the estimate from importance sampling method.
```{r}
g <- function(x) exp(-x)/(1+x^2)*(x>0)*(x<1)
f <- function(x) exp(-x)/(1-exp(-1))*(x>0)*(x<1)
M <- 10000
theta.hat <- sd.theta.hat <- numeric(2)
```
```{r}
# data generation and analysis
set.seed(1)
u <- runif(m)
x <- -log(1-u*(1-exp(-1)))
fg <- g(x)/f(x)
theta.hat[1] <- mean(fg)
sd.theta.hat[1] <-sd(fg)

# result reporting
c(theta.hat[1], sd.theta.hat[1])
```

So we have $\hat{\theta}=0.52433470$ and $se(\hat{\theta})=0.09786775$. Then we use transformation method to implement stratified importance sampling.

```{r}
k <- 5 # number of subintervals
r <- M/k # replicates per stratum
theta.hat_s <- var_s <- numeric(k)

# data generation and analysis
set.seed(1)
for (j in 1:k) {
  u <- runif(r,(j-1)/5,j/5)
  x <- -log(1-(1-exp(-1))*u)
  fg <- g(x)/k/f(x)
  theta.hat_s[j] <- mean(fg)
  var_s[j] <- var(fg)
}
theta.hat[2] <- sum(theta.hat_s)
sd.theta.hat[2] <- sqrt(sum(var_s))

# result reporting
c(theta.hat[2], sd.theta.hat[2])
```

So we have $\hat{\theta}=0.524688376$ and $se(\hat{\theta})=0.009383897$. 

```{r}
rbind(theta.hat, sd.theta.hat)
```

Comparing the sample standard error of the estimate, we can find that the stratified sampling method can effectively reduce variance.

### Exercises 6.5. (pages 180, Statistical Computing with R).

**Problem.** Suppose a 95% symmetric t-interval is applied to estimate a mean, but the sample data are non-normal. Then the probability that the confidence interval covers the mean is not necessarily equal to 0.95. Use a Monte Carlo experiment to estimate the coverage probability of the t-interval for random samples of $\chi^2(2)$ data with sample size $n = 20$. Compare your t-interval results with the
simulation results in Example 6.4. (The t-interval should be more robust to
departures from normality than the interval for variance.)

**Solution.** Firstly we use a Monte Carlo experiment to estimate the coverage probability of the t-interval for random samples of $\chi^2(2)$ data with sample size $n = 20$.

```{r}
mc <- function(n){
  t0 <- qt(c(0.025, 0.975), df = n - 1)
  x <-  rchisq(n, df = 2)
  ci <- mean(x) + t0 * sd(x)/sqrt(n)
  return(ci)
}
```

```{r}
n <- 20
set.seed(1)
CI <- replicate(10000, expr = mc(n))
LCL <- CI[1, ]
UCL <- CI[2, ]
c(mean(LCL), mean(UCL))
mean(LCL < 2 & UCL > 2)
```

Then from Example 6.6 we estimate the coverage probability of the interval for variance for random samples of $\chi^2(2)$ data with sample size $n = 20$.
```{r}
n <- 20
alpha <- 0.05

set.seed(1)
UCL <- replicate(1000, expr = {
  x <- rchisq(n, df = 2)
  (n-1) * var(x) / qchisq(alpha, df = n-1)
} )
mean(UCL)
mean(UCL>4)
```

Because 0.9133 > 0.797, we conclude that the t-interval should be more robust to departures from normality than the interval for variance.

### Exercises 6.A. (pages 181, Statistical Computing with R).

**Problem.** Use Monte Carlo simulation to investigate whether the empirical Type I error rate of the t-test is approximately equal to the nominal significance level $\alpha$, when the sampled population is non-normal. The t-test is robust to mild departures from normality. Discuss the simulation results for the cases where the sampled population is (i) $\chi^2(1)$, (ii) Uniform(0,2), and (iii) Exponential(rate=1). In each case, test $H_0:\mu=\mu_0$ vs $H_1:\mu\neq\mu_0$, where $\mu_0$ is the mean of $\chi^2(1)$, Uniform(0,2), and Exponential(1), respectively.

**Solution.** For the three cases, we calculate the $\hat{P}(p\leq\alpha)$ and $se(\hat{P}(p\leq\alpha))$ respectively.

```{r}
n <- 20
alpha <- 0.05
m <- 10000 #number of replicates
p <- matrix(0, nrow = 3, ncol = m) #storage for p-values
p.hat <- numeric(3)
se.p.hat <- numeric(3)
```

(i) $X\sim\chi^2(1)$, then $\mu_0=E(X)=1$.

```{r}
mu0 <- 1

set.seed(1)
for (j in 1:m) {
  x <- rchisq(n, df = 1)
  ttest <- t.test(x, alternative = "two.sided", mu = mu0)
  p[1,j] <- ttest$p.value
}
p.hat[1] <- mean(p[1,] < alpha)
se.p.hat[1] <- sqrt(p.hat[1] * (1 - p.hat[1]) / m)
print(c(p.hat[1], se.p.hat[1]))
```

(ii) $X\sim$Uniform(0,2), then $\mu_0=E(X)=\frac{1}{2}\int_0^2xdx=\frac{4}{4}=1$.

```{r}
mu0 <- 1

set.seed(1)
for (j in 1:m) {
  x <- runif(n, min = 0, max = 2)
  ttest <- t.test(x, alternative = "two.sided", mu = mu0)
  p[2,j] <- ttest$p.value
}
p.hat[2] <- mean(p[2,] < alpha)
se.p.hat[2] <- sqrt(p.hat[2] * (1 - p.hat[2]) / m)
print(c(p.hat[2], se.p.hat[2]))
```

(iii) $X\sim$Exponential(1), then $\mu_0=E(x)=\int_0^{\infty}xe^{-x}dx=1$.

```{r}
mu0 <- 1

set.seed(1)
for (j in 1:m) {
  x <- rexp(n, rate = 1)
  ttest <- t.test(x, alternative = "two.sided", mu = mu0)
  p[3,j] <- ttest$p.value
}
p.hat[3] <- mean(p[3,] < alpha)
se.p.hat[3] <- sqrt(p.hat[3] * (1 - p.hat[3]) / m)
print(c(p.hat[3], se.p.hat[3]))
```

```{r}
rbind(p.hat, se.p.hat)
```

We find that the empirical Type I error rate of the t-test is approximately equal to the nominal significance level $\alpha$ in case (ii), but is not in case (i) and (iii). Because Uniform(0,2) is more slightly deviates from the normal distribution than $\chi^2(1)$ and Exponential(1) as shown in the following figure.

```{r}
x <- seq(-4, 4, 0.01)
plot(x, dnorm(x, mean = 0, sd = 1), type = "l", ylim = c(0, 2), ylab = "", col = 1)
lines(x, dchisq(x, df = 1), lty = 2, col = 2)
lines(x, dunif(x, min = 0, max = 2), lty = 3, col = 3)
lines(x, dexp(x, rate = 1), lty = 4, col = 4)
legend("topleft", inset = 0.02, legend = c("N(0,1)", "Chisq(1)", "Uniform(0,2)", "Exponential(1)"), lty = 1:4, col = 1:4)
```



# Homework-2023.10.16

## Question

### Question 1.

考虑$m=1000$个假设，其中前95%个原假设成立，后5%个对立假设成立。在原假设之下，p-值服从U(0,1)分布，在对立假设之下，p-值服从Beta(0.1, 1)分布（可用rbeta生成）。应用Bonferroni矫正与B-H矫正应用于生成的$m$个p-值(独立)(应用p.adjust)，得到校正后的p-值，与$\alpha=0.1$比较确定是否拒绝原假设。基于$M=1000$次模拟，可估计FWER, FDR, TPR输出到表格中：

||FWER|FDR|TPR|
|:-:|:-:|:-:|:-:|
|Bonf| $\approx$ 0.1 | $\ll$ 0.1 |?|
|B-H| $\gg$ 0.1 | $\approx$ 0.1 |??|

### Question 2.

Homework

  + Suppose the population has the exponential distribution with rate $\lambda$, then the MLE of $\lambda$ is $\hat\lambda=1/\bar X$, where $\bar X$ is the sample mean. It can be derived that the expectation of $\hat\lambda$ is $\lambda n/(n-1)$, so that the estimation bias is $\lambda/(n-1)$. The standard error $\hat\lambda$ is $\lambda n/[(n-1)\sqrt{n-2}]$. Conduct a simulation study to verify the performance of the bootstrap method. 
  + The true value of $\lambda=2$.
  + The sample size $n=5,10,20$.
  + The number of bootstrap replicates $B = 1000$.
  + The simulations are repeated for $m=1000$ times.
  + Compare the mean bootstrap bias and bootstrap standard error with the theoretical ones. Comment on the results.

### Question 3.

Exercises 7.3 (pages 212, Statistical Computing with R).

## Answer

### Question 1.

**Solution.** We define the function "adjust" to generate and adjust the p-values as well as calculate FWER, FDR and TPR.

```{r}
adjust <- function(m, alpha){
  m0 <- m*0.95
  m1 <- m - m0
  
  # generate p-values
  p0 <- runif(m0)
  p1 <- rbeta(m1, 0.1, 1)
  p <- c(p0, p1)
  
  # adjust p-values
  p.adj_bonf = p.adjust(p, method = 'bonferroni')
  p.adj_BH = p.adjust(p, method = 'BH')
  
  # calculate FWER, FDR and TPR
  FWER_bonf <- 1 - prod(p.adj_bonf[1:m0] >= alpha)
  FWER_BH <- 1 - prod(p.adj_BH[1:m0] >= alpha)
  FDR_bonf <- sum(p.adj_bonf[1:m0] < alpha) / sum(p.adj_bonf < alpha)
  FDR_BH <- sum(p.adj_BH[1:m0] < alpha) / sum(p.adj_BH < alpha)
  TPR_bonf <- sum(p.adj_bonf[m0:m] < alpha) / m1
  TPR_BH <- sum(p.adj_BH[m0:m] < alpha) / m1
  
  return(c(FWER_bonf, FWER_BH, FDR_bonf, FDR_BH, TPR_bonf, TPR_BH))
}
```

Then we obtain the estimates of FWER, FDR and TPR from $M=1000$ simulations.
```{r}
m <- 1000
M <- 1000
alpha <- 0.1
rate <- matrix(0, nrow = M, ncol = 6)

set.seed(1)
for(i in 1:M){
  rate[i,] <- adjust(m, alpha)
}
estimate <- matrix(apply(rate, 2, mean), nrow = 2, ncol = 3)
```
```{r}
estimate <- round(estimate, 3)
colnames(estimate) <- c("FWER", "FDR", "TPR")
rownames(estimate) <- c("Bonf", "B-H")
knitr::kable(estimate)
```

From the table we find that the estimats of FWER from Bonferroni correction and FDR from B-H correction are both close to 0.1, the estimat of FWER from B-H correction is far greater than 0.1, the estimat of FDR from Bonferroni correction is far smaller that 0.1 and the estimats of TPR from Bonferroni correction is smaller than that from B-H correction.

### Question 2.

**Solution.** We calculate the mean bias and standard error of $\hat{\theta}^*$ to estimate the mean bias and standard error of $\hat{\theta}$.

```{r}
lambda <- 2 #true value
n <- c(5, 10, 20) #sample size
B <- 1000 #bootstrap replicates
m <- 1000 #simulations
lambda.star <- numeric(B)
bias.estimate <- matrix(0, nrow = m, ncol = 3)
se.estimate <- matrix(0, nrow = m, ncol = 3)
```
```{r}
set.seed(1)
for(i in 1:3){
  for(j in 1:m){
    x <- rexp(n[i], rate = 2)
    lambda.hat <- 1/mean(x)
    for(b in 1:B){
      xstar <- sample(x, replace=TRUE)
      lambda.star[b] <- 1/mean(xstar)
    }
    bias.estimate[j,i] <- mean(lambda.star)-lambda.hat
    se.estimate[j,i] <- sd(lambda.star)
  }
}
```
```{r}
bias.theta.hat <- rbind(apply(bias.estimate, 2, mean), lambda/(n-1))
se.theta.hat <- rbind(apply(se.estimate, 2, mean), lambda*n/(n-1)/sqrt(n-2))
colnames(bias.theta.hat) <- colnames(se.theta.hat) <- c("n=5", "n=10", "n=20")
rownames(bias.theta.hat) <- rownames(se.theta.hat) <- c("estimate", "theoretical")
pander::pander(bias.theta.hat)
pander::pander(se.theta.hat)
```

From the tables of the mean bootstrap bias and bootstrap standard error compared with the theoretical ones, we find that the estimates approximate the theoretical ones as the sample size increases.

### Exercises 7.3. (pages 212, Statistical Computing with R).

**Problem.**  Obtain a bootstrap t confidence interval estimate for the correlation statistic in Example 7.2 (law data in bootstrap).

**Solution.** The studentized bootstrap CI is
$$\left(\hat{\theta}-t_{1-\alpha/2}^*\hat{se}(\hat{\theta}),\hat{\theta}-t_{\alpha/2}^*\hat{se}(\hat{\theta})\right).$$
We use bootstrap method to estimate $t_a$ which is the sample a-quantile of $(\hat{\theta}^{(b)}-\hat{\theta})/\hat{se}(\hat{\theta}^{(b)})$, where $\hat{se}(\hat{\theta}^{(b)})$ involves the second order resampling.

```{r}
d <- bootstrap::law
d <- as.matrix(d)
n <- nrow(d)

B <-  1000 #bootstrap replicates for t
R <-  25 #bootstrap replicates for se
level <-  0.95
stat.star <- numeric(B)
se.star <- numeric(B)
```

```{r}
b.cor <- function(x,i = 1:nrow(x)) cor(x[i,1],x[i,2])
boot.se <- function(x, R, statistic) {
  m <- nrow(x)
  th <- replicate(R, expr = {
    i <- sample(1:m, size = m, replace = TRUE)
    statistic(x[i, ])
  })
  return(sd(th))
}
```

```{r}
set.seed(1)
for (b in 1:B) {
  j <- sample(1:n, size = n, replace = TRUE)
  xstar <- d[j, ]
  stat.star[b] <- b.cor(xstar)
  se.star[b] <- boot.se(xstar, R = R, statistic = b.cor)
#  se[b] <- sd(boot(xstar, R = R, statistic = b.cor)$t)
}
stat.hat <- b.cor(d)
t.stats <- (stat.star - stat.hat)/se.star
se.hat <- sd(stat.star)
alpha <- 1 - level
Qt <- quantile(t.stats, c(alpha/2, 1 - alpha/2), type = 1)
names(Qt) <- rev(names(Qt))
CI <- rev(stat.hat - Qt * se.hat)
CI
```



# Homework-2023.10.23

## Question

Exercises 7.5 7.8 7.11 (pages 212-213, Statistical Computing with R).

## Answer

### Exercise 7.5. (page 212, Statistical Computing with R)

**Problem.** Refer to Exercise 7.4. Compute 95% bootstrap confidence intervals for the mean time between failures $1/\lambda$ by the standard normal, basic, percentile, and BCa methods. Compare the intervals and explain why they may differ.

**Solution.** From Exercise 7.4. we know that the 12 observations in data set **aircondit** are the times in hours between failures of air-conditioning equipment, and we assume that the times between failures follow an exponential model Exp(λ). If $X\sim$Exp($\lambda$), we obtain that $E(X)=\frac{1}{\lambda}$. So the estimate $\hat{\left(\frac{1}{\lambda}\right)}=\bar{X}$.

We firstly use the function "boot" and "boot.ci" to obtain the 95% bootstrap confidence intervals by the standard normal, basic, percentile, and BCa methods.
```{r}
library(boot)
x <- aircondit[1]
mean.x <- function(x, i) return(mean(as.matrix(x[i, ])))

set.seed(1)
boot.x <- boot(x, statistic = mean.x, R = 2000)
boot.x
boot.ci(boot.x, type = c("norm", "perc", "basic", "bca"))
```

Then we plot the frequency distribution of the estimates from each boot replicate and the estimate from data.

```{r}
hist(boot.x$t, prob = TRUE, main = "", xlab = "")
points(boot.x$t0, 0, cex = 1, pch = 16)
```

```{r}
detach(package:boot)
rm(list = ls())
```

The 95% bootstrap confidence intervals for the mean time between failures $1/\lambda$ by the standard normal method is (33.2, 182.6), by the basic method is (25.2, 168.7), by the percentile method is (47.4, 190.9) and by BCa method is (57.8, 230.5).

The reasons for the difference of the intervals are as follows. From the figure we find that the replicates are not approximately normal, so the normal and percentile intervals differ. From the histogram of replicates, it appears that the distribution of the replicates is skewed, although we are estimating a mean. The reason is that the sample size is too small for CLT to give a good approximation here. The BCa interval is a percentile type interval, but it adjusts for both skewness and bias, so the BCa interval differs from the others.

### Exercise 7.8. (page 213, Statistical Computing with R).

**Problem.** Refer to Exercise 7.7. Obtain the jackknife estimates of bias and standard error of $\hat{\theta}$.

**Solution.** From Exercise 7.7. we known that the five-dimensional scores data "scor (bootstrap)" have a $5\times5$ covariance matrix $\Sigma$, with positive
eigenvalues $\lambda_1>\dots>\lambda_5$. In principal components analysis,
$$\theta=\frac{\lambda_1}{\sum_{j=1}^5\lambda_j}$$
measures the proportion of variance explained by the first principal component. Let
$\hat{\lambda}_1>\dots>\hat{\lambda}_5$ are the eigenvalues of $\hat{\Sigma}$, where $\hat{\Sigma}$ is the MLE of $\Sigma$. And the sample estimate
$$\hat{\theta}=\frac{\hat{\lambda}_1}{\sum_{j=1}^5\hat{\lambda}_j}.$$

In jackknife method, we calculate $\hat{\theta}_{(j)},j=1,\dots,n$ in the n cases $x_{(-j)},j=1,\dots,n$, and then we get $\bar{\hat{\theta}}_{(\cdot)}$ and $\hat{se}(\hat{\theta})=\sqrt{\widehat{Var}(\hat{\theta})}$.

```{r}
x <- as.matrix(bootstrap::scor)
n <- nrow(x)
theta.jack <- numeric(n)

# original theta.hat
lambda <- eigen(cov(x))$values
theta.hat <- max(lambda/sum(lambda))

# n jackknife estimates theta.jack
for (i in 1:n) {
  x.jack <- x[-i, ]
  cov.jack <- cov(x.jack)
  lambda <- eigen(cov.jack)$values
  theta.jack[i] <- max(lambda/sum(lambda))
}
bias.jack <- (n - 1) * (mean(theta.jack) - theta.hat)
se.jack <- sqrt((n - 1)/n * sum((theta.jack - mean(theta.jack))^2))

# result reporting
result <- c(theta.hat, bias.jack, se.jack)
names(result) <- c("est", "bias", "se")
pander::pander(result)
```
```{r}
rm(list = ls())
```

### Exercise 7.11. (page 213, Statistical Computing with R).

**Problem.** In Example 7.18, leave-one-out (n-fold) cross validation was used to select the best fitting model. Use leave-two-out cross validation to compare the models.

**Solution.**

```{r}
d <- DAAG::ironslag
d <- as.matrix(d)
n <- nrow(d)
N <- choose(n, 2)

e_lin <- e_quad <- e_exp <- e_loglog <- numeric(N)
ij <- 1
for (i in 1:(n - 1)) for (j in (i + 1):n) {
  k <- c(i, j)
  y <- d[-k, "magnetic"]
  x <- d[-k, "chemical"]
  
  # linear model
  J_lin <- lm(y ~ x)
  yhat_lin <- J_lin$coef[1] + J_lin$coef[2] * d[k, "chemical"]
  e_lin[ij] <- sum((d[k, "magnetic"] - yhat_lin)^2)
  
  # quadratic model
  J_quad <- lm(y ~ x + I(x^2))
  yhat_quad <- J_quad$coef[1] + J_quad$coef[2] * d[k, "chemical"] + J_quad$coef[3] * d[k, "chemical"]^2
  e_quad[ij] <- sum((d[k, "magnetic"] - yhat_quad)^2)
  
  # exponential model
  J_exp <- lm(log(y) ~ x)
  logyhat_exp <- J_exp$coef[1] + J_exp$coef[2] * d[k, "chemical"]
  yhat_exp <- exp(logyhat_exp)
  e_exp[ij] <- sum((d[k, "magnetic"] - yhat_exp)^2)
  
  # log-log model
  J_loglog <- lm(log(y) ~ log(x))
  logyhat_loglog <- J_loglog$coef[1] + J_loglog$coef[2] * log(d[k, "chemical"])
  yhat_loglog <- exp(logyhat_loglog)
  e_loglog[ij] <- sum((d[k, "magnetic"] - yhat_loglog)^2)

  ij <- ij + 1
}

# result reporting
spe <- cbind(e_lin, e_quad, e_exp, e_loglog)
aspe <- apply(spe, 2, mean)
names(aspe) <- c("linear model", "quadratic model", "exponential model", "log-log model")
pander::pander(aspe)
```
```{r}
rm(list = ls())
```

From the table of the average squared prediction error, we find that the quadratic model has the least leave-two-out error in consistent with leave-one-out validation. So the quadratic model is again selected according to the minimum prediction error by leave-two-out cross-validation.



# Homework-2023.10.30

## Question

### Question 1. 

Proof the Stationarity of Metropolis-Hastings sampler Algorithm in continuous situation.

### Question 2.

Exercises 8.1 8.3 (pages 242-243, Statistical Computing with R).

## Answer

### Question 1.

**Algorithm (continuous situation)**

  - Target pdf: $f(x)$.
  - Replace $i$ and $j$ with $s$ and $r$.
  - Proposal distribution (pdf): $g(r|s)$.
  - Acceptance probability: $\alpha(s,r)=\min\left\{\frac{f(r)g(s|r)}{f(s)g(r|s)},1\right\}$.
  - Transition kernel (mixture distribution): $K(r,s)=I_{(s\neq r)}\alpha(r,s)g(s|r)+I_{(s=r)}[1-\int\alpha(r,s)g(s|r)]$.
  - Stationarity: $K(s,r)f(s)=K(r,s)f(r)$.

**Proof.** From the definition of $K(r,s)$ and $\alpha(s,r)$, in the case that $s\neq r$, we have
$$K(s,r)f(s)=\alpha(s,r)g(r|s)f(s)=\min\left\{\frac{f(r)g(s|r)}{f(s)g(r|s)},1\right\}g(r|s)f(s)=\min\left\{f(r)g(s|r),f(s)g(r|s)\right\},$$
and
$$K(r,s)f(r)=\alpha(r,s)g(s|r)f(r)=\min\left\{\frac{f(s)g(r|s)}{f(r)g(s|r)},1\right\}g(s|r)f(r)=\min\left\{f(s)g(r|s),f(r)g(s|r)\right\},$$
so
$$K(s,r)f(s)=K(r,s)f(r).$$

In the case that $s=r$, we have
$$K(s,r)f(s)=\left[1-\int\alpha(s,r)g(r|s)\right]f(s)=\left[1-\int\min\left\{\frac{f(r)g(s|r)}{f(s)g(r|s)},1\right\}g(r|s)\right]f(s)\\
=f(s)-\int\min\{f(r)g(s|r),f(s)g(r|s)\}=f(r)-\int\min\{f(r)g(s|r),f(s)g(r|s)\}\\
=\left[1-\int\min\left\{\frac{f(s)g(r|s)}{f(r)g(s|r)},1\right\}g(s|r)\right]f(r)=\left[1-\int\alpha(r,s)g(s|r)\right]f(r)=K(r,s)f(r).$$

Combining the two cases, we obtain the result $K(s,r)f(s)=K(r,s)f(r)$.

### Exercise 8.1. (pages 242, Statistical Computing with R).

**Problem.** Implement the two-sample Cram\'er-von Mises test for equal distributions as a permutation test. Apply the test to the data in Examples 8.1 and 8.2.

**Solution.** As for two-sample tests for univariate data, we have Cram\'er-von Mises statistic
$$W_2=\frac{mn}{(m+n)^2}\left[\sum_{i=1}^n(F_n(x_i)-G_m(x_i))^2+\sum_{j=1}^m(F_n(y_j)-G_m(y_j))^2\right],$$
where $F_n$ is the ecdf of the sample $x_1,\dots,x_n$ and $G_m$ is the ecdf of the sample $y_1,\dots,y_m$.

```{r}
cvm.test <- function(x, y, R = 199) {
  n <- length(x)
  m <- length(y)
  z <- c(x, y)
  N <- n + m
  
  # Fn: ecdf of the sample x1,...,xn; Gm: ecdf of the sample y1,...,ym.
  Fn <- numeric(N)
  Gm <- numeric(N)
  for(i in 1:N){
    Fn[i] <- mean(as.integer(z[i] <= x))
    Gm[i] <- mean(as.integer(z[i] <= y))
  }
  
  # statistic W2
  cvm0 <- ((n * m)/N) * sum((Fn - Gm)^2)
  
  # p-value of the test
  cvm <- numeric(R)
  for(j in 1:R){
    index <- sample(1:N)
    Z <- z[index]
    X <- Z[1:n]
    Y <- Z[(n + 1):N]
    for (i in 1:N) {
      Fn[i] <- mean(as.integer(Z[i] <= X))
      Gm[i] <- mean(as.integer(Z[i] <= Y))
    }
    cvm[j] <- ((n * m)/N) * sum((Fn - Gm)^2)
  }
  
  # result reporting
  cvm1 <- c(cvm, cvm0)
  return(c(cvm0, mean(cvm1 >= cvm0)))
}
```

Then we calculate the statistic $M_2$ and the p-value of the test for the data from "chickwts".

```{r}
attach(chickwts)
d <- chickwts
detach(chickwts)
d <- as.matrix(d)
d[d[,2]=="horsebean", 2] <- 1
d[d[,2]=="linseed", 2] <- 2
d[d[,2]=="soybean", 2] <- 3
d[d[,2]=="sunflower", 2] <- 4
d[d[,2]=="meatmeal", 2] <- 5
d[d[,2]=="casein", 2] <- 6
```
```{r}
set.seed(100)
M2 <- matrix(NA, nrow = 6, ncol = 6)
p_value <- matrix(NA, nrow = 6, ncol = 6)
for(i in 1:5){
  for(j in (i+1):6){
    result <- cvm.test(d[d[,2]==i, 1], d[d[,2]==j, 1])
    M2[i,j] <- result[1]
    p_value[i,j] <- result[2]
  }
}
```
```{r}
rownames(M2) <- colnames(M2) <- rownames(p_value) <- colnames(p_value) <- c("horsebean", "linseed", "soybean", "sunflower", "meatmeal", "casein")
M2
p_value
```

```{r}
rm(list = ls())
```

The tests to the data on different feed of "chickwts" are shown in the table. In Example 8.1 and 8.2, the soybean and linseed groups were discussed. From the table we find that the CvM statistic $M_2=4.228$ and the p-value for the CvM test comparing soybean and linseed supplements is not significant. So there is not evidence of a difference between these distributions. In Example 8.3, the sunflower and linseed groups were discussed. From the table we find that the CvM statistic $M_2=36.5$ and The p-value for the CvM test comparing sunflower and linseed supplements is significant at $\alpha=0.01$, so there is strong evidence that the distributions of weights for these two groups are different.

### Exercise 8.3. (pages 243, Statistical Computing with R).

**Problem.** The Count 5 test for equal variances in Section 6.4 is based on the maximum number of extreme points. Example 6.15 shows that the Count 5 criterion is not applicable for unequal sample sizes. Implement a permutation test for equal variance based on the maximum number of extreme points that applies when sample sizes are not necessarily equal.

**Solution.** The two sample "Count Five" test for equality of variance counts the number of extreme points of each sample relative to the range of the other sample. The function "maxoutliers" is generated from the algorithm.

```{r}
maxoutliers <- function(x, y) {
  X <- x - mean(x)
  Y <- y - mean(y)
  outx <- sum(X > max(Y)) + sum(X < min(Y))
  outy <- sum(Y > max(X)) + sum(Y < min(X))
  return(max(c(outx, outy)))
}
```

And we calculate the "Count Five" test statistics and the p-value of the test from the function "maxout".

```{r}
maxout <- function(x, y, R = 199) {
  z <- c(x, y)
  n <- length(x)
  N <- length(z)
  
  # statistic
  stat0 <- maxoutliers(x, y)
  
  # p-value of the test
  stat <- replicate(R, expr = {
    index <- sample(1:N)
    maxoutliers(z[index[1:n]], z[index[(n+1):N]])
  })
  
  stats <- c(stat0, stat)
  tab <- table(stats)/(R + 1)
  return(list(estimate = stat0, p = mean(stats >= stat0), freq = tab, cdf = cumsum(tab)))
}
```

Through the new functions, we implement the permutation test for equal variance based on the maximum number of extreme points in four specific examples where sample sizes are not necessarily equal.

Example1. Both variances and sample sizes are equal.

```{r}
set.seed(100)
n1 <- n2 <- 30
mu1 <- mu2 <- 0
sigma1 <- sigma2 <- 1
x <- rnorm(n1, mu1, sigma1)
y <- rnorm(n2, mu2, sigma2)
maxout(x, y)
```

Example2. Variances are equal, but sample sizes are unequal.

```{r}
set.seed(100)
n1 <- 20
n2 <- 40
mu1 <- mu2 <- 0
sigma1 <- sigma2 <- 1
x <- rnorm(n1, mu1, sigma1)
y <- rnorm(n2, mu2, sigma2)
maxout(x, y)
```

Example3. Variances are unequal, but sample sizes are equal.

```{r}
set.seed(100)
n1 <- n2 <- 30
mu1 <- mu2 <- 0
sigma1 <- 1
sigma2 <- 2
x <- rnorm(n1, mu1, sigma1)
y <- rnorm(n2, mu2, sigma2)
maxout(x, y)
```

Example4. Both variances and sample sizes are unequal.
```{r}
set.seed(100)
n1 <- 20
n2 <- 40
mu1 <- mu2 <- 0
sigma1 <- 1
sigma2 <- 2
x <- rnorm(n1, mu1, sigma1)
y <- rnorm(n2, mu2, sigma2)
maxout(x, y)
```
```{r}
rm(list = ls())
```

From the results in the four examples, we find that the observed statistic is not significant in the equal variance examples, but is significant in the unequal variance examples. In conclusion, the new permutation test for equal variance based on Count 5 criterion applies when sample sizes are not necessarily equal.



# Homework-2023.11.06

## Question

### Question 1.

Consider a model $P(Y=1|X_1,X_2,X_3)=\frac{\exp(a+b_1X_1+b_2X_2+b_3X_3)}{1+\exp(a+b_1X_1+b_2X_2+b_3X_3)}$, where $X_1\sim P(1)$, $X_2\sim Exp(1)$ and $X_3\sim B(1,0.5)$.

- Design a function that takes as input values $N, b_1, b_2, b_3$ and $f_0$, and produces the output $a$.

- Call this function, input values are $N = 106, b_1 = 0, b_2 = 1, b_3 = −1, f_0 = 0.1, 0.01, 0.001, 0.0001$.

- Plot − log $f_0$ vs $a$.

### Question 2.

Exercises 9.4 9.7 9.10 (pages 277-278, Statistical Computing with R).

## Answer

### Question 1.

**Solution.** We first design the function that takes as input values $N, b_1, b_2, b_3$ and $f_0$, and produces the output $a$. The basic idea of calculating $a$ is as following:

- The prevalence $P(Y=1)$ is a function of $a$ given the other parameters and distribution of $(X_1, X_2, X_3)$, say $P(Y=1)=f(a)$.

- The problem becomes solving $f(a)=f_0$ (find the root of $f(a)-f_0=0$.

- Approximate $f(a)$ using the proportions of diseased subjects for any given $a$.

```{r}
output.a <- function(N, b, f0){
  #生成N组样本(x1i, x2i, x3i), i=1,...,N.
  x <- rbind(rpois(N, lambda = 1), rexp(N, rate = 1), sample(0:1, N, replace=TRUE))
  
  #构造函数P(Y=1)-f0用来求解a
  fa_f0 <- function(a){
    prob <- 1/(1 + exp(-a-b[1]*x[1,]-b[2]*x[2,]-b[3]*x[3,]))
    log(mean(prob)) - log(f0)
  }
  
  #利用uniroot求解f(a)-f_0=0
  solution <- uniroot(fa_f0, c(-20,0))
  return(solution$root)
}
```

Then we call this function and input values are $N = 10^6, b_1 = 0, b_2 = 1, b_3 = −1, f_0 = 0.1, 0.01, 0.001, 0.0001$.

```{r}
#按照题目要求指定 f0, b1, b2, b3, N (large enough)
N <- 1e6
b <- c(0, 1, -1)
f0 <- c(0.1, 0.01, 0.001, 0.0001)

#用前面定义的函数output.a得到a的估计
set.seed(1)
a <- sapply(f0, output.a, N = N, b = b)
names(a) <- c("f0=0.1", "f0=0.01", "f0=0.001", "f0=0.0001")
a
```

Finally we plot the graph of $\log f_0\sim a$

```{r}
estimate.logf0 <- function(a, b){
  #生成N组样本(x1i, x2i, x3i), i=1,...,N.
  set.seed(1)
  x <- rbind(rpois(N, lambda = 1), rexp(N, rate = 1), sample(0:1, N, replace=TRUE))
  
  #P(Y=1)的概率
  prob <- 1/(1 + exp(-a-b[1]*x[1,]-b[2]*x[2,]-b[3]*x[3,]))
  
  #return(log(mean(p)))
  #用二项分布抽样估计概率
  return(log(mean(rbinom(N, 1, prob))))
}
```
```{r}
#-logf0关于a的函数图像
a_axis <- seq(-12, 0, 0.1)
logf0 <- sapply(a_axis, estimate.logf0, b = b)

plot(a_axis, -logf0, type = 'l', xlab = "a", ylab = "-log f0")
for(i in 1:4){
  points(a[i], -log(f0[i]), cex = 1, pch = 16)
}
```

```{r}
rm(list = ls())
```

In the figure we show the relationship between house $-\log f_0$ and $a$. And we also find that the previous output $a$ in the case $f_0=0.1,0.01,0.001,0.0001$ matches the figure $-log f_0\sim a$.

### Exercise 9.4. (pages 277, Statistical Computing with R).

**Problem.** Implement a random walk Metropolis sampler for generating the standard Laplace distribution (see Exercise 3.2). For the increment, simulate from a normal distribution. Compare the chains generated when different variances are used for the proposal distribution. Also, compute the acceptance rates of each chain.

**Solution.** The density of standard Laplace distribution is
$$f(x)=\frac{1}{2}e^{-|x|},\ x\in\mathbb{R}.$$
And the implementation of a random walk Metropolis sampler for this problem is as follows.

- Set $g(\cdot|X)$ (proposal pdf) to be the density of $N(X,\sigma^2)$.
- Generate $X_1$ from $N(0,1)$.
- Repeat for $i=2,\dots,N$:
  - Generate $Y$ from $N(X_{i-1},\sigma^2)$.
  - Generate $U$ from Uniform(0,1).
  - Compute accept probability $\alpha(X_{i-1},Y)=\frac{f(Y)g(X_{i-1}|Y)}{f(X_{i-1})g(Y|X_{i-1})}=\frac{f(Y)}{f(X_{i-1})}=\frac{e^{-|Y|}}{e^{-|X_{i-1}|}}$. If $U\leq\alpha(X_{i-1},Y)$, accept $Y$ and set $X_i=Y$; otherwise set $X_i=X_{i-1}$.
  - Increase $i$.

```{r}
#Laplace分布密度函数
d.Laplace <- function(x){
  exp(-abs(x))/2
}

#Metropolis抽样方法，其中提议分布为正态分布
mcmc.Metropolis.normal <- function(sigma, x.initial, N, f) {
  x <- numeric(N) #M-H方法生成的样本链
  x[1] <- x.initial #初始值
  k <- 0  #记录接受Y的次数，计算接受率
  
  u <- runif(N)
  for (i in 2:N) {
    y <- rnorm(1, x[i-1], sigma)
    if (u[i] <= (f(y)/f(x[i-1]))){x[i] <- y
      k <- k + 1
    } #接受
    else{x[i] <- x[i-1]} #拒绝
  }
  return(c(x, k/N))
}
```

We get the result in the cases $\sigma=0.5,1,4,16$.
```{r}
N <- 5000 #样本链的长度
b <- 1000 #预烧期的长度
sigma <- c(0.5, 1, 4, 16) #给定的σ值
k <- length(sigma) #样本链的数量
```
```{r}
#M-H方法生成4条样本链和对应的接受率
set.seed(1)
x.initial <- rnorm(1) #初始值
mcmc.result <- sapply(sigma, mcmc.Metropolis.normal, x.initial = x.initial, N = N, f = d.Laplace)
X.chain <- t(mcmc.result[1:N,]) #样本链
accept.prob <- as.vector(mcmc.result[(N+1),]) #接受率
```

To compare the 4 chains, we show the sample path, the histogram with the true density, QQ plots and the quantiles of the generated samples for each chain.

```{r}
x_axis <- seq(-6, 6, 0.01)
x_axis.Laplace <- d.Laplace(x_axis)
qq <- seq(0.005, 1, 0.005)
qq.population <- c(-rev(qexp(qq, 1)), qexp(qq, 1))
qq.chain <- apply(X.chain[,-(1:b)], 1, function(x) quantile(x, qq))

for(i in c(0,2)){
  par(mfrow = c(2,3))
  for(j in 1:2){
    #样本路径图
    plot(X.chain[i+j,], type = "l", main = bquote(sigma == .(sigma[i+j])), xlab = "t", ylab = "X", ylim = c(-8, 8))
    
    #样本频数分布直方图
    hist(X.chain[i+j, -(1:b)], breaks = "Scott", freq = FALSE, main = bquote(sigma == .(sigma[i+j])), xlim = c(-6, 6), ylim = c(0, 0.5), xlab = "x")
    lines(x_axis, x_axis.Laplace, col = 2)
    
    #样本Q-Q图
    qqplot(qq.population, qq.chain[,i+j], cex = 0.5, main = bquote(sigma == .(sigma[i+j])), xlab = "Sample quantile", ylab = "Population quantile")
    abline(a = 0, b = 1, col = 2)
  }
}
```

```{r}
#生成服从拉普拉斯分布的样本，得到总体分位数的估计
set.seed(1)
x.Laplace <- rexp(200, 1)
x.Laplace <- x.Laplace * sample(c(-1,1), 200, replace = TRUE)
q <- c(0.05, seq(0.1, 0.9, 0.1), 0.95)
q.true <- quantile(x.Laplace, q)

#每条样本链的样本分位数
q.chain <- apply(X.chain[,-(1:b)], 1, function(x) quantile(x, q))

#比较总体分位数和样本链的样本分位数
q.table <- cbind(q.true, q.chain)
colnames(q.table) <- c('True', 'sigma=0.5', 'sigma=1', 'sigma=4', 'sigma=16')
knitr::kable(q.table)
```

From the three figures and table above, we find that the second and third case are close to the standard Laplace distribution.

```{r}
#接受率
names(accept.prob) <- c("sigma=0.5", "sigma=1", "sigma=4", "sigma=16")
accept.prob
```

```{r}
rm(list = ls())
```

As we can see, the acceptance rate of second chain (when $\sigma=1$) is more favorable. 

### Exercise 9.7. (pages 278, Statistical Computing with R).

**Problem.** Implement a Gibbs sampler to generate a bivariate normal chain $(X_t,Y_t)$ with zero means, unit standard deviations, and correlation 0.9. Plot the generated sample after discarding a suitable burn-in sample. Fit a simple linear regression model $Y=\beta_0+\beta_1X$ to the sample and check the residuals of the model for normality and constant variance.

**Solution.** The target distribution is bivariate normal that
$$(X,Y)\sim N(\mu_1,\mu_2,\sigma_1^2,\sigma_2^2,\rho).$$
So we have the conditional distributions $f(x_1|x_2)$ and $f(x_2|x_1)$ that
$$(X|Y=y)\sim N\left(\mu_1+\rho\frac{\sigma_1}{\sigma_2}(y-\mu_2), (1-\rho^2)\sigma_1^2\right),\\
(Y|X=x)\sim N\left(\mu_2+\rho\frac{\sigma_2}{\sigma_1}(x-\mu_1), (1-\rho^2)\sigma_2^2\right).$$

We firstly implement the Gibbs sampler below and plot the generated sample after discarding a suitable burn-in sample.

- Initialize $(X_1, Y_1)$.
- Repeat for $t=2,\dots,N$:
  - Generate $X_t\sim f_{X|Y}(x|Y_{t-1})$.
  - Generate $Y_t\sim f_{Y|X}(y|X_{t})$.
  - Set bivariate normal sample $(X_t,Y_t)$.
  - Increase $t$.
  
```{r}
#Gibbs方法估计二元正态分布
mcmc.Gibbs.binormal <- function(mu, sigma, rho, initial, N) {
  X <- matrix(0, nrow = 2, ncol = N) #样本链
  X[,1] <- initial #初始值
  s <- sqrt(1 - rho^2) * sigma #条件正态分布的标准差
  
  for (t in 2:N) {
    mu1 <- mu[1] + rho*(X[2, t-1] - mu[2])*sigma[1]/sigma[2]
    X[1, t] <- rnorm(1, mu1, s[1])#从Y_{t-1}得到X_{t}
    mu2 <- mu[2] + rho*(X[1, t] - mu[1])*sigma[2]/sigma[1]
    X[2, t] <- rnorm(1, mu2, s[2])#从X_{t}得到Y_{t}
  }
  return(X)
}
```

```{r}
N <- 5000 #length of chain
b <- 1000 #burn-in length
rho <- 0.9
mu <- c(0, 0)
sigma <- c(1, 1)

#用Gibbs方法得到样本链
set.seed(1)
mcmc.result <- mcmc.Gibbs.binormal(mu, sigma, rho, mu, N)
#样本链X和Y的协方差
cov(t(mcmc.result))
```

The covariance of $(X_t,Y_t)$ is close to the theoretical matrix $Cov(X,Y)$. And after discarding the burn-in sample, the scatter diagram of the generated chain is elliptically symmetric and centered at the origin

```{r}
#样本链X和Y的散点图
X.chain <- mcmc.result[1,]
Y.chain <- mcmc.result[2,]
plot(X.chain[-(1:b)], Y.chain[-(1:b)], xlab = "X", ylab = "Y", main = "Scatter diagram of (X,Y)", cex = 0.5, ylim = c(-5,5))
abline(h = 0, v = 0, lty = 2)
```

From the sample covariance and scatter diagram, we can conclude that the distribution of the chains $(X_t,Y_t)$ approaches to the true distribution. Then we fit a linear regression model $Y=\beta_0+\beta_1X$ to the sample.

```{r}
#Y关于X的线性回归模型
lin.reg <- lm(Y.chain[-(1:b)] ~ X.chain[-(1:b)])
lin.reg
summary(lin.reg)
```

The coefficients of the fitted model 0.903291 is close to the true value 0.9. Then we check whether the model's residuals are normal and constant variance. The theoretical value of the variance of $e$ is
$$Var(e)=Var(Y-0.9X)=Var(Y)+0.9^2Var(X)-2\cdot0.9Cov(X,Y)=1+0.81-1.8\cdot0.9=0.19.$$
So we compare the distribution of the residuals with normal distribution $N(0,\sigma^2),\ \sigma^2=0.19$.

```{r}
#残差的期望和方差的理论值和估计值对比
mean.var <- c(0, mean(lin.reg$residuals), 0.19, var(lin.reg$residuals))
names(mean.var) <- c("theoretical mean", "estimated mean", "theoretical var", "estimated var")
pander::pander(mean.var)
```

From the comparison we find that the mean and variance of residuals are close to the theoretical values.

```{r}
par(mfrow = c(1, 2))
#残差的频数分布直方图
x_axis <- seq(-2, 2, 0.01)
hist(lin.reg$residuals, breaks = "Scott", freq = FALSE, main = "Density of residuals", xlim = c(-2, 2), ylim = c(0, 1), xlab = "Residual", ylab = "Density")
lines(x_axis, dnorm(x_axis, 0, sqrt(0.19)), col = 2)

#残差的Q-Q图
qqnorm(lin.reg$residuals, cex = 0.5)
qqline(lin.reg$residuals, col = 2)
```

```{r}
rm(list = ls())
```

From the histogram and QQ-plot, we can conclude that the residual is from the normal distribution with constant variance close to the theoretical value.

### Exercise 9.10. (pages 278, Statistical Computing with R).

**Problem.** Refer to Example 9.1. Use the Gelman-Rubin method to monitor convergence of the chain, and run the chain until the chain has converged approximately to the target distribution according to $\hat{R}<1.2$. (See Exercise 9.9.) Also use the **coda** [212] package to check for convergence of the chain by the Gelman-Rubin method. Hints: See the help topics for the coda functions **gelman.diag**, **gelman.plot**, **as.mcmc**, and **mcmc.list**.

**Solution.** In Example 9.1, the Metropolis-Hastings with proposal distribution $\chi^2$(fd = $Y$) is used to sampler to generate a sample from a Rayleigh distribution whose density is
$$f(x)=\frac{x}{\sigma^2}e^{-\frac{x^2}{2\sigma^2}},\ x\geq0,\sigma>0.$$

```{r}
#Raleigh分布的密度
d.Raleigh <- function(x, sigma){
  if (any(x >= 0)){
    return((x/sigma^2) * exp(-x^2/(2*sigma^2)))
  }else{return(0)}
}

#M-H方法，其中提议分布为卡方分布
mcmc.MH.chisq <- function(sigma, N, x.initial, f){
  x <- numeric(N) #M-H方法生成的样本链
  x[1] <- x.initial #初始值
  u <- runif(N)
  
  for (i in 2:N) {
    y <- rchisq(1, df = x[i-1])
    numerator <- f(y, sigma) * dchisq(x[i-1], df = y)
    denominator <- f(x[i-1], sigma) * dchisq(y, df = x[i-1])
    if (u[i] <= numerator/denominator){
      x[i] <- y #接受
    }else{x[i] <- x[i-1]} #拒绝
  }
  return(x)
}
```

In the Gelman-Rubin method, we define that $\{X_{ij},1\leq i\leq k,1\leq j\leq n\}$ are $k$ chains of length $n$ and $\phi_{in}=\phi(X_{i1},\dots,X_{in}),i=1,\dots,k$ are scalar summary statistics that estimates some parameter of the target distribution. The Gelman-Rubin statistic is defined as
$$\hat{R}=\frac{\widehat{Var}(\phi)}{W_n},$$
where
$$\widehat{Var}(\phi)=\frac{n-1}{n}W_n+\frac{1}{n}B_n,\ W_n=\frac{1}{k}\sum_{i=1}^k\frac{1}{n-1}\sum_{j=1}^n(\bar{\phi}_{ij}-\bar{\phi}_{i\cdot})^2,\ B_n=\frac{n}{k-1}\sum_{i=1}^k(\bar{\phi}_{i\cdot}-\bar{\phi}_{\cdot\cdot})^2.$$

```{r}
#Gelman-Rubin方法
Gelman.Rubin <- function(phi) {
  n <- ncol(phi)
  
  #计算Bn, Wn
  phi.i.mean <- apply(phi, 1, mean)
  B <- n * var(phi.i.mean)
  phi.i.var <- apply(phi, 1, var)
  W <- mean(phi.i.var)
  
  #计算Var.hat, R.hat
  var.hat <- W*(n - 1)/n + B/n
  r.hat <- var.hat/W
  return(r.hat)
}
```

We firstly use the above function to monitor convergence of the chain, and run the chain until the chain has converged approximately to the target distribution according to $\hat{R}<1.2$.

```{r}
sigma <- 4
x.initial <- c(0.5, 1, 4, 16) #给定初始值
k <- length(x.initial) #样本链的数量
N <- 2000 #样本链的长度
b <- 500 #预烧期的长度

#用M-H方法生成样本链
set.seed(1)
X.chain <- sapply(x.initial, mcmc.MH.chisq, sigma = sigma, N = N, f = d.Raleigh)
#处理样本链得到Φ_{ij}为第i个链前j项的均值
phi.sum <- t(apply(X.chain, 2, cumsum))
phi.mean <- t(apply(phi.sum, 1, function(x) x/(1:N)))
```

```{r}
#用Gelman-Rubin方法监测收敛
r.hat.chain <- rep(0, N)
for (j in (b+1):N){
  r.hat.chain[j] <- Gelman.Rubin(phi.mean[, 1:j])
}

#R.hat的图像
plot(r.hat.chain[(b+1):N], type = "l", xlab = "t", ylab = "R.hat")
abline(h = 1.2, lty = 2)
#N=2000时的R.hat
r.hat.final <- Gelman.Rubin(phi.mean)
r.hat.final
```

Then we use the **coda** package to check for convergence of the chain by the Gelman-Rubin method. From help topics, we know that the function **as.mcmc** is used to convert other data structures into a Markov Chain Monte Carlo object. The function **mcmc.list** is used to represent parallel runs of the same chain, with different starting values and random seeds. The function **gelman.diag** gives the Gelman and Rubin's convergence diagnostic. The plot from function **gelman.plot** shows the evolution of Gelman and Rubin's shrink factor as the number of iterations increases.

We firstly convert the chains into mcmc objects and combine them as a list. Then we use **gelman.diag** and **gelman.plot** to discuss the convergence of chains.

```{r}
library(coda)
```

```{r}
X.chain <- t(X.chain)
#为了使用coda中的Gelman-Rubin诊断函数，我们将链的数据结构转换为mcmc型
mcmc.chain <- mcmc.list(as.mcmc(X.chain[1, ]), as.mcmc(X.chain[2, ]), as.mcmc(X.chain[3, ]), as.mcmc(X.chain[4, ]))
#Gelman-Rubin诊断
gelman.diag(mcmc.chain)
gelman.plot(mcmc.chain)
```

```{r}
detach(package:coda)
rm(list = ls())
```

From the above results we can conclude that the MCMC chains tend to be stable.



# Homework-2023.11.13

## Question

### Question 1. 

**课后习题**

设
$$X_1,\dots,X_n\overset{\mathrm{i.i.d}}{\sim}\mathrm{Exp}(\lambda).$$
因为某种原因，只知道$X_i$落在某个区间$(u_i,v_i)$，其中$u_i<v_i$是两个非随机的未知常数。这种数据称为区间删失数据。

1. 试分别极大化观测数据的似然函数与采用EM算法求解$\lambda$的MLE，证明EM算法收敛于观测数据的MLE，且收敛有线性速度。

2. 设$(u_i,v_i),i=1,\dots,n(=10)$的观测值为
$$(11,12),(8,9),(27,28),(13,14),(16,17),(0,1),(23,24),(10,11),(24,25),(2,3),$$
试分别编程实现上述两种算法以得到$\lambda$的MLE的数值解。

**提示：**观测数据的似然函数为$L(\lambda)=\prod_{i=1}^n\mathrm{P}_{\lambda}(u_i\leq X_i\leq v_i)$。

### Question 2.

Exercises 11.8 (pages 354, Statistical Computing with R).

## Answer

### Question 1.1.

**Solution.** (1). Firstly we obtain the MLE of $\lambda$ through the observed data likelihood function. From the density and distribution function of Exp($\lambda$) which are
$$f(x|\lambda)=\lambda e^{-\lambda x}\quad \mathrm{and}\quad F(x|\lambda)=\mathrm{P}_{\lambda}(X\leq x)=1-e^{-\lambda x},$$
the observed data likelihood function equals that
$$L_o(\lambda|\mathbf{u},\mathbf{v})=\prod_{i=1}^nL_o(\lambda|u_i,v_i)=\prod_{i=1}^n\mathrm{P}_{\lambda}(u_i\leq X_i\leq v_i)=\prod_{i=1}^n\left((1-e^{-\lambda v_i})-(1-e^{\lambda u_i})\right)=\prod_{i=1}^n\left(e^{-\lambda u_i}-e^{-\lambda v_i}\right).$$
Then the corresponding observed data logarithmic likelihood function is that
$$l_o(\lambda|\mathbf{u},\mathbf{v})=\log\left(L_o(\lambda|\mathbf{u},\mathbf{v})\right)=\sum_{i=1}^n\log\left(e^{-\lambda u_i}-e^{-\lambda v_i}\right).$$

To get the MLE $\hat{\lambda}_{MLE}:=\max_{\lambda>0}l_o(\lambda|\mathbf{u},\mathbf{v})$, we obtain the maximum point of $l_o(\lambda|\mathbf{u},\mathbf{v})$ by exploring the first and second derivatives of it. Because
$$\dot{l}_o(\lambda|\mathbf{u},\mathbf{v})=\frac{\partial l_o(\lambda|\mathbf{u},\mathbf{v})}{\partial\lambda}=\sum_{i=1}^n\frac{v_ie^{-\lambda v_i}-u_ie^{-\lambda u_i}}{e^{-\lambda u_i}-e^{-\lambda v_i}},$$
we have
$$\lim_{\lambda\to0+}\dot{l}_o(\lambda|\mathbf{u},\mathbf{v})=\lim_{\lambda\to0+}\sum_{i=1}^n\frac{v_ie^{-\lambda(v_i-u_i)}-u_i}{1-e^{-\lambda(v_i-u_i)}}=+\infty,\\ \lim_{\lambda\to+\infty}\dot{l}_o(\lambda|\mathbf{u},\mathbf{v})=\lim_{\lambda\to0+}\sum_{i=1}^n\frac{v_ie^{-\lambda(v_i-u_i)}-u_i}{1-e^{-\lambda(v_i-u_i)}}=\sum_{i=1}^n-u_i<0.$$
Combining the above result with the fact that
\begin{align*}
  \ddot{l}_o(\lambda|\mathbf{u},\mathbf{v})=&\frac{\partial^2 l_o(\lambda|\mathbf{u},\mathbf{v})}{\partial\lambda^2}=\sum_{i=1}^n\frac{\left(u_i^2e^{-\lambda u_i}-v_i^2e^{-\lambda v_i}\right)\left(e^{-\lambda u_i}-e^{-\lambda v_i}\right)-\left(v_ie^{-\lambda v_i}-u_ie^{-\lambda u_i}\right)^2}{\left(e^{-\lambda u_i}-e^{-\lambda v_i}\right)^2}\\
  =&\sum_{i=1}^n\frac{\left(u_i^2e^{-2\lambda u_i}-u_i^2e^{-\lambda(u_i+v_i)}-v_i^2e^{-\lambda(u_i+v_i)}+v_i^2e^{-2\lambda v_i}\right)-\left(u_i^2e^{-2\lambda u_i}-2u_iv_ie^{-\lambda(u_i+v_i)}+v_i^2e^{-2\lambda v_i}\right)}{\left(e^{-\lambda u_i}-e^{-\lambda v_i}\right)^2}\\
  =&\sum_{i=1}^n\frac{-u_i^2e^{-\lambda(u_i+v_i)}-v_i^2e^{-\lambda(u_i+v_i)}+2u_iv_ie^{-\lambda(u_i+v_i)}}{\left(e^{-\lambda u_i}-e^{-\lambda v_i}\right)^2}=-\sum_{i=1}^n\frac{(u_i-v_i)^2e^{-\lambda(u_i+v_i)}}{\left(e^{-\lambda u_i}-e^{-\lambda v_i}\right)^2}<0
\end{align*}

when $u_i<v_i,i=1,\dots,n$, we have known the function $\dot{l}_o(\lambda|\mathbf{u},\mathbf{v})$ is strictly monotonically decreasing and has a unique zero point on $(0,+\infty)$. So $\hat{\lambda}_{MLE}$ is the unique solution of the function
$$\dot{l}_o(\lambda|\mathbf{u},\mathbf{v})=\sum_{i=1}^n\frac{v_ie^{-\lambda v_i}-u_ie^{-\lambda u_i}}{e^{-\lambda u_i}-e^{-\lambda v_i}}=0$$
on $(0,+\infty)$, which maximizes the function ${l}_o(\lambda|\mathbf{u},\mathbf{v})$.




(2). Secondly we obtain the estimate of $\lambda$ through the EM algorithm. We assume the complete data likelihood function and logarithmic likelihood function are
$$L_c(\lambda|\mathbf{x})=\prod_{i=1}^nf(x_i|\lambda)=\prod_{i=1}^n\lambda e^{-\lambda x_i}=\lambda^ne^{-\lambda\sum_{i=1}^nx_i},\\
l_c(\lambda|\mathbf{x})=\log\left(L_c(\lambda|\mathbf{x})\right)=n\log\lambda-\lambda\sum_{i=1}^nx_i.$$

The algorithm process is shown below.

1. Initiate $\lambda$ with $\lambda_0$.

2. E-step: For $t\in\mathbb{N}$, from
\begin{align*}
  x_{ti}=&\mathbb{E}_{\lambda_t}[x_i|x_i\in(u_i,v_i)]=\frac{\mathbb{E}_{\lambda_t}[x_iI_{x_i\in(u_i,v_i)}]}{\mathbb{E}_{\lambda_t}[I_{x_i\in(u_i,v_i)}]}=\frac{\int_{u_i}^{v_i}xf(x|\lambda_t)dx}{F(v_i|\lambda_t)-F(u_i|\lambda_t)}=\frac{\int_{u_i}^{v_i}\lambda_txe^{-\lambda_tx}dx}{e^{-\lambda_tu_i}-e^{-\lambda_tv_i}}\\
  =&\frac{-xe^{-\lambda_tx}|_{u_i}^{v_i}+\int_{u_i}^{v_i}e^{-\lambda_tx}dx}{e^{-\lambda_tu_i}-e^{-\lambda_tv_i}}=\frac{-v_ie^{-\lambda_tv_i}+u_ie^{-\lambda_tu_i}-\frac{1}{\lambda_t}e^{-\lambda_tx}|_{u_i}^{v_i}}{e^{-\lambda_tu_i}-e^{-\lambda_tv_i}}=\frac{-v_ie^{-\lambda_tv_i}+u_ie^{-\lambda_tu_i}-\frac{1}{\lambda_t}e^{-\lambda_tv_i}+\frac{1}{\lambda_t}e^{-\lambda_tu_i}}{e^{-\lambda_tu_i}-e^{-\lambda_tv_i}}\\
  =&\frac{u_ie^{-\lambda_tu_i}-v_ie^{-\lambda_tv_i}}{e^{-\lambda_tu_i}-e^{-\lambda_tv_i}}+\frac{1}{\lambda_t},
\end{align*}
we have
\begin{align*}
l_t(\lambda|\mathbf{u},\mathbf{v})=&\mathbb{E}_{\lambda_t}\left[l_c(\lambda|\mathbf{x})|\mathbf{u},\mathbf{v}\right]=\mathbb{E}_{\lambda_t}\left[\left.n\log\lambda-\lambda\sum_{i=1}^nx_i\right|x_i\in(u_i,v_i),i=1,\dots,n\right]=n\log\lambda-\lambda\sum_{i=1}^nx_{ti}\\
  =&n\log\lambda-\lambda\sum_{i=1}^n\left(\frac{u_ie^{-\lambda_tu_i}-v_ie^{-\lambda_tv_i}}{e^{-\lambda_tu_i}-e^{-\lambda_tv_i}}+\frac{1}{\lambda_t}\right)=n\log\lambda-n\frac{\lambda}{\lambda_t}+\lambda\dot{l}_o(\lambda_t|\mathbf{u},\mathbf{v}).
\end{align*}

3. M-step: maximum $l_t(\lambda|\mathbf{u},\mathbf{v})$ w.r.t. $\lambda$, let the resulting
maximizer be $\lambda_{t+1}$. Because
$$\dot{l}_t(\lambda|\mathbf{u},\mathbf{v})=\frac{\partial l_t(\lambda|\mathbf{u},\mathbf{v})}{\partial\lambda}=\frac{n}{\lambda}-\frac{n}{\lambda_t}+\dot{l}_o(\lambda_t|\mathbf{u},\mathbf{v})$$
and
$$\ddot{l}_t(\lambda|\mathbf{u},\mathbf{v})=-\frac{n}{\lambda^2}<0,$$
we find that the function $l_t(\lambda|\mathbf{u},\mathbf{v})$ has a unique maximum point $\lambda_{t+1}$ which satisfies the equation that
$$\frac{n}{\lambda_{t+1}}=\frac{n}{\lambda_t}-\dot{l}_o(\lambda_t|\mathbf{u},\mathbf{v}).$$

4. Repeat steps 2 and 3 until the sequence $\{\lambda_t\},t=0,1,\dots$ convergence.




(3). Finally we prove that the sequence $\{\lambda_t\}_{t=0}^{+\infty}$ from EM algorithm converges to the MLE $\hat{\lambda}_{MLE}$ of the observed data and has a linear rate of convergence. 

Define the function
$$g(y)=y-\frac{1}{n}\dot{l}_o\left(\left.\frac{1}{y}\right|\mathbf{u},\mathbf{v}\right)=\frac{1}{n}\sum_{i=1}^ny-\frac{v_ie^{-v_i/y}-u_ie^{-u_i/y}}{e^{-u_i/y}-e^{-v_i/y}}=\frac{1}{n}\sum_{i=1}^ny-\frac{v_ie^{-(v_i-u_i)/y}-u_i}{1-e^{-(v_i-u_i)/y}}=\frac{1}{n}\sum_{i=1}^n\frac{h(y;u_i,v_i)}{1-e^{-(v_i-u_i)/y}},\quad y\in(0,+\infty),$$
where
$$h(y;u_i,v_i)=y(1-e^{-(v_i-u_i)/y})-(v_ie^{-(v_i-u_i)/y}-u_i).$$
Then we have $\frac{1}{\lambda_{t+1}}=g\left(\frac{1}{\lambda_t}\right)$. Because
$$\dot{h}(y;u_i,v_i)=\frac{\partial h(y;u_i,v_i)}{\partial y}=1-e^{-(v_i-u_i)/y}+(y+v_i)\frac{(v_i-u_i)}{y^2}e^{-(v_i-u_i)/y}>0,$$
when $y\in(0,+\infty)$, and
$$\lim_{y\to0+}h(y;u_i,v_i)=u_i>0,$$
we obtain that $g(y)|_{y\in(0,+\infty)}\subset(0,+\infty)$，which means that $g(y)$ is a self mapping function from $(0,+\infty)$ to $(0,+\infty)$.

To obtain the Laplace value of the function $g(y)$, we explore the properties of the derivative of $g(y)$ with respect to $y$ that
$$g'(y)=1+\frac{1}{n}\ddot{l}_o\left(\left.\frac{1}{y}\right|\mathbf{u},\mathbf{v}\right)\frac{1}{y^2}=\frac{1}{n}\sum_{i=1}^n1-\frac{(u_i-v_i)^2e^{-(u_i+v_i)/y}}{y^2\left(e^{-u_i/y}-e^{-v_i/y}\right)^2}\triangleq\frac{1}{n}\sum_{i=1}^nl(y;u_i,v_i).$$

Because
$$\frac{(u-v)^2e^{-(u+v)/y}}{y^2\left(e^{-u/y}-e^{-v/y}\right)^2}=\frac{(v-u)^2}{y^2(e^{-u/y}-e^{-v/y})(e^{v/y}-e^{u/y})}=\frac{(v-u)^2}{y^2(e^{(v-u)/y}-2+e^{-(v-u)/y})}=\left(\frac{2\frac{v-u}{2y}}{e^{\frac{v-u}{2y}}-e^{-\frac{v-u}{2y}}}\right)^2,$$
we only need to discuss the function
$$s(y)=\frac{2y}{e^y-e^{-y}},\quad y\in(0,+\infty).$$
Because
$$s'(y)=2\frac{(e^y-e^{-y})-y(e^y+e^{-y})}{(e^y-e^{-y})^2}=2\frac{(1-y)e^y-(y+1)e^{-y}}{(e^y-e^{-y})^2}$$
whose numerator $t(y)=(1-y)e^y-(y+1)e^{-y}$ has the derivative
$$t'(y)=-ye^y+ye^{-y}=-y(e^y-e^{-y})<0,\ y\in(0,+\infty),$$
and thus has the maximum $t(0)=1-1=0$, the derivative $s'(y)<0$ on $(0,+\infty)$ and $s(y)$ strictly monotone decreasing on $(0,+\infty)$. So the function $s(y)$ takes the  upper and lower bounds on $(0,+\infty)$ if and only if
$$\sup_{y\in(0,+\infty)}s(y)=\lim_{y\to0+}s(y)=\lim_{y\to0+}\frac{2y}{1+y-(1-y)+O(y^2)}=1,\quad \inf_{y\in(0,+\infty)}s(y)=\lim_{y\to+\infty}s(y)=0.$$
As a result, the function $l(y;u_i,v_i)$ decreases strictly monotonically with respect to $y$ on $(0,+\infty)$ and
$$\sup_{y\in(0,+\infty)}l(y;u_i,v_i)=\lim_{y\to0+}l(y;u_i,v_i)=1,\quad \inf_{y\in(0,+\infty)}l(y;u_i,v_i)=\lim_{y\to+\infty}l(y;u_i,v_i)=0.$$

From the discussion in (1), we have known that there exists a fixed point $y_0\in(0,+\infty)$ of the  function $g(y)=y-\frac{1}{n}\dot{l}_o\left(\left.\frac{1}{y}\right|\mathbf{u},\mathbf{v}\right)$ which satisfies the function $\dot{l}_o\left(\left.\frac{1}{y}\right|\mathbf{u},\mathbf{v}\right)=0$. For any $\epsilon\in(0,y_0)$, define functions $g_{\epsilon}(y)$ and $l_{\epsilon}(y;u_i,v_i)$ that have the same expression as $g(y)$ and $l(y;u_i,v_i)$ but with the different domain $[\epsilon,+\infty)$. Because $\sup_{y\in[\epsilon,+\infty)}l(y;u_i,v_i)=l(\epsilon;u_i,v_i)<1$, we have
$$k_{\epsilon}\triangleq\sup_{y\in[\epsilon,+\infty)}|g_{\epsilon}'(y)|\leq\frac{1}{n}\sum_{i=1}^n\sup_{y\in[\epsilon,+\infty)}l(y;u_i,v_i)=\frac{1}{n}\sum_{i=1}^nl(\epsilon;u_i,v_i)<1.$$
So for any $y_1,y_2\in[\epsilon,+\infty)$, $|g(y_1)-g(y_2)|\leq k_{\epsilon}|y_i-y_2|$. Additionally, for $\forall y\in[\epsilon,+\infty)$, if $y\in[y_0,+\infty)$, $g_{\epsilon}(y)\geq g_{\epsilon}(y_0)=y_0>\epsilon$; if $y\in[\epsilon,y_0)$, $g_{\epsilon}(y_0)-g_{\epsilon}(y)\leq k_{\epsilon}(y_0-y)<y_0-y$, then $g_{\epsilon}(y)>g_{\epsilon}(y_0)-y_0+y=y\geq\epsilon$, so $g_{\epsilon}(y)\in[\epsilon ,+\infty)$. Thus we find that function $g_{\epsilon}(y)$ is both self mapping and contraction mapping. From the Banach’s Fixed Point Theory, there exists a unique fixed point of $g_{\epsilon}(y)$, which means that there exists a unique fixed point of $g(y)$ on $[\epsilon,+\infty)$. Because of the arbitrariness of $\epsilon$, we conclude that there exists a unique fixed point of $g(y)$ on $(0,+\infty)$.

What's more, we generate a sequence $\{x_i\}_{i=1}^{+\infty}$ from any initial value $x_0\in(0,+\infty)$ and the iteration $x_{i}=g(x_{i-1}),i\in\mathbb{N}^*$. Through the Banach’s Fixed Point Theory for $g_{\epsilon}(y)$ where $\epsilon\leq\min(x_0,y_0)$, we have that the sequence $\{x_i\}_{i=1}^{+\infty}\subset[\epsilon,+\infty)$ converges to $y_0$. And from the above discussion we have known that $g'(y_0)\in(0,\infty)$. Thus the sequence $\{x_i\}_{i=1}^{+\infty}$ has a linear rate of convergence.

Let $\hat{\lambda}_{MLE}=\frac{1}{y_0}$ and $\lambda_t=\frac{1}{x_t},t\in\mathbb{N}$. We conclude that the sequence $\{\lambda_t\}_{t=0}^{+\infty}$ from EM algorithm converges to the MLE $\hat{\lambda}_{MLE}$ of the observed data and has a linear rate of convergence. 

### Question 1.2.

**Solution.** As for the first method, we obtain the MLE $\hat{\lambda}_{MLE}$ of $\lambda$ through the observed data likelihood function, which is the unique solution of the function
$$\dot{l}_o(\lambda|\mathbf{u},\mathbf{v})=\sum_{i=1}^n\frac{v_ie^{-\lambda v_i}-u_ie^{-\lambda u_i}}{e^{-\lambda u_i}-e^{-\lambda v_i}}=0$$
on $(0,+\infty)$.

Secondly we obtain the estimate of $\lambda$ through the EM algorithm with iteration
$$\frac{n}{\lambda_{t+1}}=\frac{n}{\lambda_t}-\dot{l}_o(\lambda_t|\mathbf{u},\mathbf{v}).$$

```{r}
#观测数据对数似然的一阶导函数
d.log.likelihood_o <- function(lambda, u, v){
  numerator <- v*exp(-lambda*v) - u*exp(-lambda*u)
  denominator <- exp(-lambda*u) - exp(-lambda*v)
  return(sum(numerator/denominator))
}

#观测数据：x的下界
u <- c(11,8,27,13,16,0,23,10,24,2)
#观测数据：x的下界
v <- c(12,9,28,14,17,1,24,11,25,3)
#存储两种方法得到的估计值
lambda.estimate <- numeric(2)
```

```{r}
#方法1：关于观测数据似然函数的MLE
#求解对数似然方程
result <- uniroot(d.log.likelihood_o, interval = c(0.01,0.1), u=u, v=v)
result
lambda.estimate[1] <- as.numeric(result[1])
```

```{r}
#方法2：EM算法
n <- length(u) #样本容量
N <- 1e3 #迭代次数上界
lambda.seq <- numeric(N+1) #λ数列
lambda.seq[1] <- 0.01 #初始化
rate <- .Machine$double.eps #当数列中元素变化足够小判断收敛
#迭代
for(i in 1:N){
  lambda.seq[i+1] <- n/(n/lambda.seq[i] - d.log.likelihood_o(lambda.seq[i],u,v))
  if ((abs(lambda.seq[i+1] - lambda.seq[i])/lambda.seq[i]) < rate){
    cat("The sequence converges after", i, "iterations.", '\n')
    break
  }
}
lambda.estimate[2] <- lambda.seq[i+1]
```

```{r}
#数据整理
names(lambda.estimate) <- c("MLE","EM")
lambda.estimate
```
```{r}
rm(list = ls())
```

The estimates of $\lambda$ from the two methods are similar.

### Exercises 11.8 (pages 354, Statistical Computing with R).

**Problem.** In the Morra game, the set of optimal strategies are not changed if a constant is subtracted from every entry of the payoff matrix, or a positive constant is multiplied times every entry of the payoff matrix. However, the simplex algorithm may terminate at a different basic feasible point (also optimal). Compute **B <- A + 2**, find the solution of game $B$, and verify that it is one of the extreme points (11.12)-(11.15) of the original game $A$. Also find the value of game $A$ and game $B$.

**Solution.** We generate the function solve.game according to Example 11.17.
```{r}
Morra.game <- function(A) {
  #用单纯形法求解二人零和博弈
  
  #对A进行线性变换使得A中元素属于[0,1]，且目标函数v >= 0
  original.A <- A
  A_min <- min(A)
  A <- A - A_min #由此可得A中元素 >= 0，于是v >= 0
  A_max <- max(A)
  A <- A / A_max #由此可得A中元素属于[0,1]
  m.x <- nrow(A)
  n.y <- ncol(A)
  iteration <- n.y^3 #在单纯形法的每个阶段要进行的最大迭代次数
  
  #先优化玩家1，再优化玩家2
  
  #设玩家1的策略为x^*=(x_1,...,x_m)^T，把v作为额外变量，即要求最小值的目标函数，令x=(x_1,..., x_m, x_{m+1}=v)^T。最大化v=x_{m+1}=e_{m+1}^T x，受制于A1x <= 0, A3x = 1。
  object.x <- c(rep(0, m.x), 1) #目标函数：收益矩阵的值v=x_{m+1}=object.x^T x
  A1.x.leq <- -cbind(t(A), rep(-1, n.y)) #A1是由-A的转置与元素全为1的列合并得到的n*(m+1)维矩阵，即A1=-(A^T -1_n)
  b1.x.leq <- rep(0, n.y) #约束A_1x <= 0
  A3.x.eq <- t(as.matrix(c(rep(1, m.x), 0))) #A3是前m项为1，第m+1项为0的矩阵，即A3=(1_m^T 0)
  b3.x.eq <- 1 #约束A3x = 1，即sum(x^*)=1
  #利用函数simplex求解，解得x=(x_1,..., x_m, x_{m+1}=v)
  simplex.x <- simplex(a=object.x, A1=A1.x.leq, b1=b1.x.leq, A3=A3.x.eq, b3=b3.x.eq, maxi=TRUE, n.iter=iteration)
  
  #设玩家2的策略为y^*=(y_1,...,y_n)，把v作为额外变量，即要求最小值的目标函数，令y=(y_1,..., y_n, y_{n+1}=v)^T。最大化v=y_{n+1}=e_{n+1}^T y，受制于A1y <= 0, A3y = 1。
  object.y <- c(rep(0, n.y), 1) #目标函数：收益矩阵的值v=y_{n+1}=object.y^T y
  A1.y.leq <- cbind(A, rep(-1, m.x)) #A1是由A与元素全为-1的列合并得到的m*(n+1)维矩阵，即A1=(A 1_m)
  b1.y.leq <- rep(0, m.x) #约束A_1y <= 0
  A3.y.eq <- t(as.matrix(c(rep(1, n.y), 0))) #A3是前n项为1，第n+1项为0的矩阵，即A3=(1_n^T 0)
  b3.y.eq <- 1 #约束A3y = 1，即sum(y^*)=1
  #利用函数simplex求解，解得y=(x_1,..., y_n, y_{n+1}=v)
  simplex.y <- simplex(a=object.y, A1=A1.y.leq, b1=b1.y.leq, A3=A3.y.eq, b3=b3.y.eq, maxi=FALSE, n.iter=iteration)
  
  #结果整理，以列表形式输出收益矩阵，最优策略，以及Morra游戏的值
  result <- list("payoff matrix" = original.A, "strategy.x" = simplex.x$soln[1:m.x], "strategy.y" = simplex.y$soln[1:n.y], "value" = simplex.x$soln[m.x+1] * A_max + A_min)
  return(result)
}
```

And then we get the optimal strategies and the values of games $A$ and $B$ through the new function, and discuss which one of the extreme points (11.12)-(11.15) equals the optimal strategy of game $A$ and game $B$.

```{r}
A <- matrix(c(0,-2,-2,3,0,0,4,0,0,
              2,0,0,0,-3,-3,4,0,0,
              2,0,0,3,0,0,0,-4,-4,
              -3,0,-3,0,4,0,0,5,0,
              0,3,0,-4,0,-4,0,5,0,
              0,3,0,0,4,0,-5,0,-5,
              -4,-4,0,0,0,5,0,0,6,
              0,0,4,-5,-5,0,0,0,6,
              0,0,4,0,0,5,-6,-6,0), nrow = 9, ncol = 9) #收益矩阵
B <- A + 2 #收益矩阵每个元素都增加一个常数
library(boot) #包含函数simplex的包
#对A和B分别使用函数Morra.game，得到最优策略及Morra游戏的值
result.A <- Morra.game(A)
result.B <- Morra.game(B)
```

```{r}
#结果整理
#比较游戏A和B的最优策略
compare.strategy <- round(cbind(result.A$strategy.x, result.A$strategy.y, result.B$strategy.x, result.B$strategy.y), 8)
colnames(compare.strategy) <- c("strategy 1 for A", "strategy 2 for A", "strategy 1 for B", "strategy 2 for B")
knitr::kable(compare.strategy)

#观察最优策略是式(11.12)-(11.15)中的哪一个
extreme.pionts <- cbind(rep(0,4), rep(0,4), c(5/12,16/37,20/47,25/61), rep(0,4), c(4/12,12/37,15/47,20/61), rep(0,4), c(3/12,9/37,12/47,16/61), rep(0,4), rep(0,4))
difference <- extreme.pionts - matrix(rep(compare.strategy[,1],4), nrow = 4, ncol = 9, byrow = TRUE)
#apply(extreme.pionts, 1, function(x) prod(all.equal(x,compare.strategy[,1])))
#apply(extreme.pionts, 1, function(x) identical(x,compare.strategy[,1]))
as.logical(apply(abs(difference)<1e-8, 1, prod))

#比较游戏A和B的值
compare.value <- c(result.A$value, result.B$value)
names(compare.value) <- c("value of game A", "value of game B")
pander::pander(compare.value)
```

```{r}
detach(package:boot)
rm(list = ls())
```

From above comparison between the solutions from solving game $A$ and $B$, we find that the optimal strategies of game $A$ and $B$ are the same and both close to the extreme point given by (11.15), and the difference between the value of game $B$ and the value of game $A$ equals the difference between the elements of payoff matrix $B$ and $A$. As a result, we conclude that if a constant is subtracted from each entry of the payoff matrix, the optimal policy set does not change, and the decrease of the value of the game is equal to the subtracted constant.



# Homework-2023.11.20

## Question

### Question 1.

- 2.1.3 Exercise 4 (Pages 19 Advanced in R)
- 2.3.1 Exercise 1, 2 (Pages 26 Advanced in R)
- 2.4.5 Exercise 2, 3 (Pages 30 Advanced in R)
- Exercises 2 (page 204, Advanced R)
- Exercises 1 (page 213, Advanced R)

### Question 2.

Consider Exercise 9.8 (pages 278, Statistical Computing with R). (Hint: Refer to the first example of Case studies section)

- Write an R function.
- Write an Rcpp function.
- Compare the computation time of the two functions with the function “microbenchmark”

## Answer

## Question 1.

### 2.1.3 Exercise 4 (Pages 19 Advanced in R).

**Problem.** Why do you need to use unlist() to convert a list to an atomic vector? Why doesn’t as.vector() work?

**Solution.** Use a simple example to see the difference of the function "unlist" and "as.vector".
```{r}
test.list <- list(1:3)
test.list; str(test.list)
as.vector(test.list); str(as.vector(test.list))
1:3; str(1:3)
unlist(test.list); str(unlist(test.list))
```
```{r}
rm(list = ls())
```

From the example we find that given a list structure, the result given by function "as.vector()" equals the original list, but the result given by function "unlist()" equals the vector consists of the elements in the list.

The reason is that the generic definition of "vector" in R contains not only an atomic vector but also other data structures such as "list". The function "unlist()" is specifically for list structure, which simplifies a list to produce a vector which contains all the atomic components which occur in it. But "as.vector()" is a generic function that still convert a list(vector) to a vector(list).

### 2.3.1 Exercise 1 (Pages 26 Advanced in R).

**Problem.** What does dim() return when applied to a vector?

**Solution.**

```{r}
test.vector <- 1:3; str(test.vector)
dim(test.vector)
dim(matrix(test.vector, nrow = 1, ncol = 3))
```
```{r}
rm(list = ls())
```

From the example we find that if a vector is entered, the function "dim()" returns NULL, which is different from the case when a matrix is entered. Because "dim()" is a function that only for matrix, array, and data frame.

### 2.3.1 Exercise 2 (Pages 26 Advanced in R).

**Problem.** If is.matrix(x) is TRUE, what will is.array(x) return?

**Solution.**

```{r}
test.matrix <- matrix(1:3, nrow = 1, ncol = 3)
is.matrix(test.matrix)
is.array(test.matrix)
```
```{r}
rm(list = ls())
```

From the example we find that if a data structure returns TRUE after "is.matrix()", it will also return TRUE after "is.array()". The reason is that matrix structure belongs to the array structure and can be regarded as a two-dimensional array.

### 2.4.5 Exercise 2 (Pages 30 Advanced in R).

**Problem.** What does as.matrix() do when applied to a data frame with columns of different types?

**Solution.** We generate a data frame containing different types of data structures such as int, numeric, logical, character, matrix and list. And we will see what will be output after we input this data frame in as.matrix().

```{r}
test.matrix <- matrix(1:8, nrow = 4, ncol = 2)
colnames(test.matrix) <- c("col1", "col2") 
#只要行数与数据框匹配，数据框的列也可以是矩阵或数组。
#但是这里需要命名矩阵的列，否则运行as.matrix()会由于列数和列名长度不同而报错。

test.dataframe <- data.frame(a = 1:4, b = c(1.0,2.0,3.0,4.0), c = c(TRUE, FALSE, TRUE, FALSE), d = c("a", "b", "c", "d"), e = test.matrix, f = I(list(1:2, 1:3, c(1.0,2.0), c(1:2,3.0))), g = I(list(1:2, c("a", "b", "c"), c(TRUE, FALSE), test.matrix)))
test.dataframe; str(test.dataframe)
as.matrix(test.dataframe); str(as.matrix(test.dataframe))
```
```{r}
rm(list = ls())
```

This result is consistent with the description of the function "as.matrix()" that is a generic function. The method for data frames will return a character matrix if there is only atomic columns and any non-(numeric/logical/complex) column, applying "as.vector()" to factors and format to other non-character columns. Otherwise, the usual coercion hierarchy (logical < integer < double < complex) will be used, e.g., all-logical data frames will be coerced to a logical matrix, mixed logical-integer will give a integer matrix, etc.

What's more, the print method for a matrix gives a rectangular layout with dimnames or indices. For a list matrix, the entries of length not one are printed in the form "integer,2"⁠ indicating the type and length.

### 2.4.5 Exercise 3 (Pages 30 Advanced in R).

**Problem.** Can you have a data frame with 0 rows? What about 0 columns?

**Solution.** Yes. We can generate a data frame with 0 rows by converting a matrix with 0 rows into a data frame. We can also generate a data frame with 0 columns by converting a matrix with 0 columns to a data frame or by simply setting the row.names of the data frame when using the function "data.frame()".

```{r}
#使用函数data.frame()时只设置col.names无法生成0行的数据框。
row0.dataframe <- data.frame(col.names = c('a','b'))
row0.dataframe; dim(row0.dataframe)

#使用函数data.frame()时只设置row.names可以生成0列的数据框。
col0.dataframe <- data.frame(row.names = c('a','b'))
col0.dataframe; dim(col0.dataframe)
```

```{r}
#通过将一个0行的矩阵转换为数据框，生成一个0行的数据框。
row0.matrix <- matrix(nrow = 0, ncol = 2)
row0.dataframe <- data.frame(row0.matrix)
row0.dataframe; dim(row0.dataframe)

#通过将一个0列的矩阵转换为数据框，生成一个0列的数据框。
col0.dataframe <- data.frame(t(row0.matrix))
col0.dataframe; dim(col0.dataframe)
```
```{r}
rm(list = ls())
```

### Exercises 2 (page 204, Advanced R).

**Problem.** The function below scales a vector so it falls in the range [0, 1]. How would you apply it to every column of a data frame?
How would you apply it to every numeric column in a data frame?
```{r}
scale01 <- function(x) {
  rng <- range(x, na.rm = TRUE)
  (x - rng[1]) / (rng[2] - rng[1])
}
```

**Solution.** Firstly, for a data frame consists of numeric column, we use the function "lapply()" to apply function "scale01()" to every column of it.

```{r}
#生成一个每列都是int或double型的数据框
set.seed(1)
numeric.dataframe <- data.frame(a = 1:5, b = c(5.0, 4.0, 3.0, 2.0, 1.0), c = rnorm(5))
numeric.dataframe
#对数据框的每一列使用函数scale01()
data.frame(lapply(numeric.dataframe, scale01))
```

Secondly, for a data frame containing different types of data structures int, numeric, logical, character and list, we use the function "lapply()" to apply function "scale01()" to every numeric column of it.

```{r}
#生成列为int, double, logical, character或list的数据框
mixed.dataframe <- data.frame(a = 1:4, b = c(1.0,2.0,3.0,4.0), c = c(TRUE, FALSE, TRUE, FALSE), d = c("a", "b", "c", "d"), e = I(list(1:2, 1:3, c(1.0,2.0), c(1:2,3.0))), f = I(list(1:2, c("a", "b", "c"), c(TRUE, FALSE), matrix(1:4, nrow = 2, ncol = 2))))
mixed.dataframe
#对数据框的每一个numeric型的列使用函数scale01()
scale.dataframe <- data.frame(lapply(mixed.dataframe, function(x) if (is.numeric(x)) scale01(x) else x))
scale.dataframe; str(scale.dataframe)
```
```{r}
rm(list = ls())
```

### Exercises 1 (page 213, Advanced R).

**Problem.** Use vapply() to:

a. Compute the standard deviation of every column in a numeric data frame.
b. Compute the standard deviation of every numeric column in a mixed data frame. (Hint: you’ll need to use vapply() twice.)

**Solution.** (a). For a numeric data frame, we use the function "vapply()" to apply function "sd()" to every column of it.

```{r}
#生成一个每个元素都是numeric型的数据框
set.seed(1)
numeric.dataframe <- data.frame(a = 1:4, b = c(4.0, 3.0, 2.0, 1.0), c = rnorm(4))
numeric.dataframe
#对数据框的每一列使用函数sd()
vapply(numeric.dataframe, sd, FUN.VALUE = 1)
```

(b). For a mixed data frame containing different types of data structures int, numeric, logical, character and list, we use the function "vapply()" to apply function "sd()" to every numeric column of it.

```{r}
#生成列为int, double, logical, character或list的数据框
mixed.dataframe <- data.frame(a = 1:4, b = c(4.0,3.0,2.0,1.0), c = rnorm(4), d = c(TRUE, FALSE, TRUE, FALSE), e = c("a", "b", "c", "d"), f = I(list(1:2, 1:3, c(1.0,2.0), c(1:2,3.0))), g = I(list(1:2, c("a", "b", "c"), c(TRUE, FALSE), matrix(1:4, nrow = 2, ncol = 2))))
mixed.dataframe
#先判断数据框的numeric型的列有哪些，然后对每一个numeric型的列使用函数sd()
numeric.col <- vapply(mixed.dataframe, is.numeric, FUN.VALUE = TRUE)
vapply(mixed.dataframe[,numeric.col], sd, FUN.VALUE = 1)
```

## Question 2.

**Exercise 9.8 (pages 278, Statistical Computing with R).** This example appears in [40]. Consider the bivariate density
$$f(x,y)\propto\binom{n}{x}y^{x+a-1}(1-y)^{n-x+b-1},\quad x=0,\dots,n,\ 0\leq y\leq1.$$
It can be shown (see e.g. [23]) that for fixed $a, b, n$, the conditional distributions are Binomial$(n, y)$ and Beta$(x + a, n − x + b)$. Use the Gibbs sampler to generate a chain with target joint density $f(x, y)$.

**Solution.** From the target bivariate density, we have the marginal distribution of random variables that
$$f_X(x)\propto\int_0^1f(x,y)dy=\binom{n}{x}\mathrm{Beta}(x+a,n-x+b)=\binom{n}{x}\frac{\Gamma(x+a)\Gamma(n-x+b)}{\Gamma(n+a+b)},\\
f_Y(y)\propto\sum_{x=0}^{n}f(x,y)=y^{a-1}(1-y)^{b-1}\sum_{x=0}^n\binom{n}{x}y^x(1-y)^{n-x}=y^{a-1}(1-y)^{b-1}(y+1-y)^n=y^{a-1}(1-y)^{b-1}.$$
So the conditional distributions are
$$f_{X|Y}(x|y)=\frac{f(x,y)}{f(y)}=\binom{n}{x}y^x(1-y)^{n-x},\\
f_{Y|X}(y|x)=\frac{f(x,y)}{f(x)}=\frac{1}{\mathrm{Beta}(x+a,n-x+b)}y^{x+a-1}(1-y)^{n-x+b-1},$$
which means that
$$(X|Y=y)\sim\mathrm{Binomial}(n,y),\quad (Y|X=x)\sim\mathrm{Beta}(x+a,n-x+b).$$

We implement the Gibbs sampler through the Gibbs method.

- Initialize $(X_1, Y_1)$.
- Repeat for $t=2,\dots,N$:
  - Generate $X_t\sim f_{X|Y}(x|Y_{t-1})$, i.e. $(X_t|Y_{t-1})\sim$\mathrm{Binomial}$(n,Y_{t-1})$.
  - Generate $Y_t\sim f_{Y|X}(y|X_{t})$, i.e. $(Y_t|X_t)\sim\mathrm{Beta}(X_t+a,n-X_t+b)$.
  - Set bivariate random sample $(X_t,Y_t)$.
  - Increase $t$.

### R function

We generate a R function to implement the Gibbs sampler.
```{r}
#Gibbs方法估计二元分布
mcmc.Gibbs.R <- function(a, b, n, initial, N) {
  #a, b, n: 二元分布参数
  #initial: 初始值
  #N: 需要生成的样本链长度
  X <- matrix(0, nrow = 2, ncol = N) #样本链
  X[,1] <- initial #赋初值
  
  for (t in 2:N){
    #从Y_{t-1}得到X_{t}，(X_t|Y_{t-1})服从Binomial(n,Y_{t-1})分布
    X[1, t] <- rbinom(1, size = n, prob = X[2, t-1])
    #从X_{t}得到Y_{t}，(Y_t|X_t)服从Beta(X_t+a,n-X_t+b)分布
    X[2, t] <- rbeta(1, shape1 = X[1, t] + a, shape2 =  n - X[1, t] + b)
  }
  return(X)
}
```

### Rcpp function

We generate a Rcpp function to implement the Gibbs sampler.

```{c,eval=FALSE}
# include <Rcpp.h>
using namespace Rcpp;

// [[Rcpp::export]]
NumericMatrix mcmc_Gibbs_Rcpp(double a, double b, double n, NumericVector initial, int N){
  //a, b, n: 二元分布参数
  //initial: 初始值
  //N: 需要生成的样本链长度
  NuemricMatrix X(2, N); //样本链
  X(0, 0) = initial(0); //赋初值
  X(1, 0) = initial(1);
  
  for (int t = 1; t < N; t++){
    //从Y_{t-1}得到X_{t}，(X_t|Y_{t-1})服从Binomial(n,Y_{t-1})分布
    X(0, t) = Rcpp::rbinom(1, size = n, prob = X(1, t-1))[0];
    //从X_{t}得到Y_{t}，(Y_t|X_t)服从Beta(X_t+a,n-X_t+b)分布
    X(1, t) = Rcpp::rbeta(1, shape1 = X(0, t) + a, shape2 =  n - X(0, t) + b)[0];
  }
  return(X);
}
```

```{r}
library(Rcpp)
```

### Comparison of the results

Then we use the function to generate Gibbs chains in the case that $a=2, b=5, n=10$. And we compare the distributions of the Gibbs chains for random variables X and Y respectively with the marginal distribution of them that
$$f_X(x)=\binom{n}{x}\frac{\mathrm{Beta}(x+a,n-x+b)}{\mathrm{Beta}(a,b)},\\
f_Y(y)=\frac{1}{\mathrm{Beta}(a,b)}y^{a-1}(1-y)^{b-1}.$$

```{r}
N <- 5000 #样本链长度
B <- 1000 #预烧期长度
a <- 2; b <- 5; n <- 9 #二元分布参数
initial <- c(1, 0.5)

#用R函数实现Gibbs方法得到样本链
set.seed(1)
mcmc.result.R <- mcmc.Gibbs.R(a, b, n, initial, N)
X.chain.R <- mcmc.result.R[1, -(1:B)]
Y.chain.R <- mcmc.result.R[2, -(1:B)]

#用Rcpp函数实现Gibbs方法得到样本链
set.seed(1)
sourceCpp('../src/mcmc.Gibbs.Rcpp.cpp')
mcmc.result.Rcpp <- mcmc_Gibbs_Rcpp(a, b, n, initial, N)
X.chain.Rcpp <- mcmc.result.Rcpp[1, -(1:B)]
Y.chain.Rcpp <- mcmc.result.Rcpp[2, -(1:B)]
```

```{r}
#列表比较关于X的样本链的分布和X的理论边缘分布
#X样本链分布
X.chain.R.freq <- table(X.chain.R) / (N-B)
X.chain.Rcpp.freq <- table(X.chain.Rcpp) / (N-B)
#X理论分布
X.value <- 0:9
X.population <- choose(n, X.value) * beta(X.value+a, n-X.value+b) / beta(a, b)

X.compare <- rbind(X.chain.R.freq, X.chain.Rcpp.freq, X.population)
rownames(X.compare) <- c("Samples from R function", "Samples from Rcpp function", "Theoretical distribution")
round(X.compare,4)
```
```{r}
par(mfrow = c(1,2))
#作图比较关于由R函数生成的X的样本链的分布和X的理论边缘分布
barplot(X.chain.R.freq, space = 0, main = "Density of X from R function", xlim = c(0, 9), ylim = c(0, 0.25), xlab = "X", ylab = "Density") #X样本链分布
points(0:n + 0.5, X.population, col = 2, pch = 20) #X理论分布

#作图比较关于由R函数生成的Y的样本链的分布和Y的理论边缘分布
Y_axis <- seq(0,1,0.01)
hist(Y.chain.R, breaks = "Scott", freq = FALSE, main = "Density of Y from R function", xlim = c(0, 1), ylim = c(0, 2.5), xlab = "Y", ylab = "Density") #Y样本链分布
lines(Y_axis, Y_axis^(a-1) * (1-Y_axis)^(b-1) / beta(a,b), col = 2) #Y理论分布

par(mfrow = c(1,2))
#作图比较关于由Rcpp函数生成的X的样本链的分布和X的理论边缘分布
barplot(X.chain.Rcpp.freq, space = 0, main = "Density of X from Rcpp function", xlim = c(0, 9), ylim = c(0, 0.25), xlab = "X", ylab = "Density") #X样本链分布
points(0:n + 0.5, X.population, col = 2, pch = 20) #X理论分布

#作图比较关于由Rcpp函数生成的Y的样本链的分布和Y的理论边缘分布
Y_axis <- seq(0,1,0.01)
hist(Y.chain.Rcpp, breaks = "Scott", freq = FALSE, main = "Density of Y from Rcpp function", xlim = c(0, 1), ylim = c(0, 2.5), xlab = "Y", ylab = "Density") #Y样本链分布
lines(Y_axis, Y_axis^(a-1) * (1-Y_axis)^(b-1) / beta(a,b), col = 2) #Y理论分布
```

As we can see, the Gibbs chains generated from R function and Rcpp function are the same because them are generated under the same seed. And we can conclude that the distributions of Gibbs chains for random variables X and Y are close to the theoretical distributions.

### Comparison of the computation time

We compare the computation time of the two functions.

```{r}
library(microbenchmark)
compare.time <- microbenchmark(R.function = mcmc.Gibbs.R(a, b, n, initial, N), Rcpp.function = mcmc_Gibbs_Rcpp(a, b, n, initial, N))
summary(compare.time)[,c(1,3,5,6)]
```
```{r}
detach(package:microbenchmark)
rm(list = ls())
```

From the comparison above, we find that the computation speed of Rcpp function is much faster than the R function. This supports the claim that using Rcpp in R allows for more efficient computation.


